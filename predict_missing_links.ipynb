{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Node-information\" data-toc-modified-id=\"Node-information-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Node information</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#String-processing\" data-toc-modified-id=\"String-processing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>String processing</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Neighbors\" data-toc-modified-id=\"Neighbors-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Neighbors</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Bag-of-Words-Abstract\" data-toc-modified-id=\"Bag-of-Words-Abstract-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Bag of Words Abstract</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Node2vec-embeddings\" data-toc-modified-id=\"Node2vec-embeddings-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Node2vec embeddings</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#node2vec\" data-toc-modified-id=\"node2vec-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>node2vec</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Set-of-features-$F$\" data-toc-modified-id=\"Set-of-features-$F$-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Set of features $F$</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Combination-of-node2vec-and-features-$F$\" data-toc-modified-id=\"Combination-of-node2vec-and-features-$F$-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Combination of node2vec and features $F$</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Fine-tuning-the-set-$F$\" data-toc-modified-id=\"Fine-tuning-the-set-$F$-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Fine-tuning the set $F$</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Comparison-of-classifiers\" data-toc-modified-id=\"Comparison-of-classifiers-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Comparison of classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Random-Forest\" data-toc-modified-id=\"Random-Forest-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Gradient-boosting\" data-toc-modified-id=\"Gradient-boosting-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Gradient boosting</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Gradient-Boosting-hyperparameter-optimization\" data-toc-modified-id=\"Gradient-Boosting-hyperparameter-optimization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Gradient Boosting hyperparameter optimization</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/ngsa-predict-missing-links/predict_missing_links.ipynb#Predictions\" data-toc-modified-id=\"Predictions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Predictions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the columns when showing DataFrames\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to identify missing citations in a citation network of research articles. A citation network is represented as a graph G=(V, E), where the nodes correspond to scientific articles and the existence of a directed edge between nodes u and v, indicates that paper u cites paper v.\n",
    "\n",
    "Each node (i.e., article) is also associated with information such as the title of the paper, publication year, author names and a short abstract. A number of edges have been randomly removed from the original citation network.\n",
    "\n",
    "Your goal is to accurately reconstruct the initial network using graph-theoretical and textual features, and possibly other information. Your solution can be based on supervised or unsupervised techniques for link prediction or on a combination of both. You should aim for the maximum F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don't go over this section if the file `data/node_information_completed.csv` has already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_text, format_author_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for each node\n",
    "with open(\"data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "df_node_info = pd.DataFrame(node_info, columns=[\"paper_id\", \"year\", \"title\", \n",
    "                                                \"authors\", \"journal_name\", \"abstract\"])\n",
    "df_node_info.index = df_node_info.paper_id\n",
    "df_node_info.drop(\"paper_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the year column to numeric setting\n",
    "df_node_info[\"year\"] = df_node_info.year.map(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info[\"title_processed\"] = df_node_info[\"title\"].map(lambda t: process_text(t))\n",
    "\n",
    "df_node_info[\"authors_processed\"] = df_node_info[\"authors\"].map(\n",
    "                                    lambda l: [author for author in l.split(\",\") \n",
    "                                               if author not in (\" \", \"\")])\n",
    "\n",
    "df_node_info[\"abstract_processed\"] = df_node_info[\"abstract\"].map(lambda t: process_text(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the author names to be consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the author names\n",
    "df_node_info[\"authors_processed\"] = df_node_info[\"authors_processed\"]\\\n",
    "                                  .apply(lambda x: format_author_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>authors_processed</th>\n",
       "      <th>abstract_processed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[P S Aspinwall]</td>\n",
       "      <td>[notes, based, lectures, given, tasi99, review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, walls, massive, gauged, supergravity,...</td>\n",
       "      <td>[M Cvetic, H Lu, C N Pope]</td>\n",
       "      <td>[point, massive, gauged, supergravity, potenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuations, brane, worlds]</td>\n",
       "      <td>[Y S Myung, G Kang]</td>\n",
       "      <td>[recently, ivanov, volovich, hep-th, 9912242, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[moving, mirrors, thermodynamic, paradoxes]</td>\n",
       "      <td>[A D Helfer]</td>\n",
       "      <td>[quantum, fields, responding, moving, mirrors,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundles, chiral, blocks, boundary, conditions...</td>\n",
       "      <td>[J Fuchs, C Schweigert]</td>\n",
       "      <td>[proceedings, lie, iii, clausthal, july, 1999,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year                                              title  \\\n",
       "paper_id                                                            \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                              authors       journal_name  \\\n",
       "paper_id                                                   \n",
       "1001                Paul S. Aspinwall                      \n",
       "1002      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "1003         Y.S. Myung, Gungwon Kang                      \n",
       "1004                   Adam D. Helfer          Phys.Rev.   \n",
       "1005          J. Fuchs, C. Schweigert                      \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "1001      these are notes based on lectures given at tas...   \n",
       "1002      we point out that massive gauged supergravity ...   \n",
       "1003      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "1004      quantum fields responding to moving mirrors ha...   \n",
       "1005      proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                                            title_processed  \\\n",
       "paper_id                                                      \n",
       "1001                  [compactification, geometry, duality]   \n",
       "1002      [domain, walls, massive, gauged, supergravity,...   \n",
       "1003         [comment, metric, fluctuations, brane, worlds]   \n",
       "1004            [moving, mirrors, thermodynamic, paradoxes]   \n",
       "1005      [bundles, chiral, blocks, boundary, conditions...   \n",
       "\n",
       "                   authors_processed  \\\n",
       "paper_id                               \n",
       "1001                 [P S Aspinwall]   \n",
       "1002      [M Cvetic, H Lu, C N Pope]   \n",
       "1003             [Y S Myung, G Kang]   \n",
       "1004                    [A D Helfer]   \n",
       "1005         [J Fuchs, C Schweigert]   \n",
       "\n",
       "                                         abstract_processed  \n",
       "paper_id                                                     \n",
       "1001      [notes, based, lectures, given, tasi99, review...  \n",
       "1002      [point, massive, gauged, supergravity, potenti...  \n",
       "1003      [recently, ivanov, volovich, hep-th, 9912242, ...  \n",
       "1004      [quantum, fields, responding, moving, mirrors,...  \n",
       "1005      [proceedings, lie, iii, clausthal, july, 1999,...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_node_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we get the direct neighbors for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "with open(\"data/training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "df_train = pd.DataFrame(training_set, columns=[\"source\", \"target\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct neighbors for each source node\n",
    "df_neighbors = df_train.groupby(\"source\")[\"target\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info = df_node_info.join(df_neighbors)\n",
    "df_node_info.rename(columns={\"target\": \"neighbors\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of neighbors \n",
    "df_node_info[\"neighbors\"] = df_node_info[\"neighbors\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>authors_processed</th>\n",
       "      <th>abstract_processed</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[P S Aspinwall]</td>\n",
       "      <td>[notes, based, lectures, given, tasi99, review...</td>\n",
       "      <td>[9603161, 9702094, 9703082, 9507158, 9404151, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, walls, massive, gauged, supergravity,...</td>\n",
       "      <td>[M Cvetic, H Lu, C N Pope]</td>\n",
       "      <td>[point, massive, gauged, supergravity, potenti...</td>\n",
       "      <td>[9912012, 9812035, 9902155, 9606076, 9809015, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuations, brane, worlds]</td>\n",
       "      <td>[Y S Myung, G Kang]</td>\n",
       "      <td>[recently, ivanov, volovich, hep-th, 9912242, ...</td>\n",
       "      <td>[205120, 9409090, 211182, 9912242, 208125, 970...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[moving, mirrors, thermodynamic, paradoxes]</td>\n",
       "      <td>[A D Helfer]</td>\n",
       "      <td>[quantum, fields, responding, moving, mirrors,...</td>\n",
       "      <td>[7129, 9305170, 9608190, 209232, 9910133, 1041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundles, chiral, blocks, boundary, conditions...</td>\n",
       "      <td>[J Fuchs, C Schweigert]</td>\n",
       "      <td>[proceedings, lie, iii, clausthal, july, 1999,...</td>\n",
       "      <td>[9908025, 9909114, 4153, 9805026, 9307074, 960...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year                                              title  \\\n",
       "paper_id                                                            \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                              authors       journal_name  \\\n",
       "paper_id                                                   \n",
       "1001                Paul S. Aspinwall                      \n",
       "1002      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "1003         Y.S. Myung, Gungwon Kang                      \n",
       "1004                   Adam D. Helfer          Phys.Rev.   \n",
       "1005          J. Fuchs, C. Schweigert                      \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "1001      these are notes based on lectures given at tas...   \n",
       "1002      we point out that massive gauged supergravity ...   \n",
       "1003      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "1004      quantum fields responding to moving mirrors ha...   \n",
       "1005      proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                                            title_processed  \\\n",
       "paper_id                                                      \n",
       "1001                  [compactification, geometry, duality]   \n",
       "1002      [domain, walls, massive, gauged, supergravity,...   \n",
       "1003         [comment, metric, fluctuations, brane, worlds]   \n",
       "1004            [moving, mirrors, thermodynamic, paradoxes]   \n",
       "1005      [bundles, chiral, blocks, boundary, conditions...   \n",
       "\n",
       "                   authors_processed  \\\n",
       "paper_id                               \n",
       "1001                 [P S Aspinwall]   \n",
       "1002      [M Cvetic, H Lu, C N Pope]   \n",
       "1003             [Y S Myung, G Kang]   \n",
       "1004                    [A D Helfer]   \n",
       "1005         [J Fuchs, C Schweigert]   \n",
       "\n",
       "                                         abstract_processed  \\\n",
       "paper_id                                                      \n",
       "1001      [notes, based, lectures, given, tasi99, review...   \n",
       "1002      [point, massive, gauged, supergravity, potenti...   \n",
       "1003      [recently, ivanov, volovich, hep-th, 9912242, ...   \n",
       "1004      [quantum, fields, responding, moving, mirrors,...   \n",
       "1005      [proceedings, lie, iii, clausthal, july, 1999,...   \n",
       "\n",
       "                                                  neighbors  \n",
       "paper_id                                                     \n",
       "1001      [9603161, 9702094, 9703082, 9507158, 9404151, ...  \n",
       "1002      [9912012, 9812035, 9902155, 9606076, 9809015, ...  \n",
       "1003      [205120, 9409090, 211182, 9912242, 208125, 970...  \n",
       "1004      [7129, 9305170, 9608190, 209232, 9910133, 1041...  \n",
       "1005      [9908025, 9909114, 4153, 9805026, 9307074, 960...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_node_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Word2vec, BoV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings = r\"C:\\Users\\Nasser Benab\\Documents\\school\\m.v.a\\deep_learning\\mini_projects\\mp2\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we encode the abstract using BoV embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(path_embeddings, \"crawl-300d-200k.vec\"), \n",
    "               nmax=200000)\n",
    "s2v = BoV(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info[\"abstract_embedding\"] = [embd for embd \n",
    "                                      in s2v.encode(df_node_info.abstract_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>authors_processed</th>\n",
       "      <th>abstract_processed</th>\n",
       "      <th>neighbors</th>\n",
       "      <th>abstract_embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[P S Aspinwall]</td>\n",
       "      <td>[notes, based, lectures, given, tasi99, review...</td>\n",
       "      <td>[9603161, 9702094, 9703082, 9507158, 9404151, ...</td>\n",
       "      <td>[0.016762745098039215, 0.15632156862745095, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, walls, massive, gauged, supergravity,...</td>\n",
       "      <td>[M Cvetic, H Lu, C N Pope]</td>\n",
       "      <td>[point, massive, gauged, supergravity, potenti...</td>\n",
       "      <td>[9912012, 9812035, 9902155, 9606076, 9809015, ...</td>\n",
       "      <td>[0.04356931818181819, 0.1502068181818182, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuations, brane, worlds]</td>\n",
       "      <td>[Y S Myung, G Kang]</td>\n",
       "      <td>[recently, ivanov, volovich, hep-th, 9912242, ...</td>\n",
       "      <td>[205120, 9409090, 211182, 9912242, 208125, 970...</td>\n",
       "      <td>[-0.012898113207547173, 0.23114905660377363, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[moving, mirrors, thermodynamic, paradoxes]</td>\n",
       "      <td>[A D Helfer]</td>\n",
       "      <td>[quantum, fields, responding, moving, mirrors,...</td>\n",
       "      <td>[7129, 9305170, 9608190, 209232, 9910133, 1041...</td>\n",
       "      <td>[0.005588000000000004, 0.15716799999999997, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundles, chiral, blocks, boundary, conditions...</td>\n",
       "      <td>[J Fuchs, C Schweigert]</td>\n",
       "      <td>[proceedings, lie, iii, clausthal, july, 1999,...</td>\n",
       "      <td>[9908025, 9909114, 4153, 9805026, 9307074, 960...</td>\n",
       "      <td>[0.08252173913043478, 0.15854347826086954, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year                                              title  \\\n",
       "paper_id                                                            \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                              authors       journal_name  \\\n",
       "paper_id                                                   \n",
       "1001                Paul S. Aspinwall                      \n",
       "1002      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "1003         Y.S. Myung, Gungwon Kang                      \n",
       "1004                   Adam D. Helfer          Phys.Rev.   \n",
       "1005          J. Fuchs, C. Schweigert                      \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "1001      these are notes based on lectures given at tas...   \n",
       "1002      we point out that massive gauged supergravity ...   \n",
       "1003      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "1004      quantum fields responding to moving mirrors ha...   \n",
       "1005      proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                                            title_processed  \\\n",
       "paper_id                                                      \n",
       "1001                  [compactification, geometry, duality]   \n",
       "1002      [domain, walls, massive, gauged, supergravity,...   \n",
       "1003         [comment, metric, fluctuations, brane, worlds]   \n",
       "1004            [moving, mirrors, thermodynamic, paradoxes]   \n",
       "1005      [bundles, chiral, blocks, boundary, conditions...   \n",
       "\n",
       "                   authors_processed  \\\n",
       "paper_id                               \n",
       "1001                 [P S Aspinwall]   \n",
       "1002      [M Cvetic, H Lu, C N Pope]   \n",
       "1003             [Y S Myung, G Kang]   \n",
       "1004                    [A D Helfer]   \n",
       "1005         [J Fuchs, C Schweigert]   \n",
       "\n",
       "                                         abstract_processed  \\\n",
       "paper_id                                                      \n",
       "1001      [notes, based, lectures, given, tasi99, review...   \n",
       "1002      [point, massive, gauged, supergravity, potenti...   \n",
       "1003      [recently, ivanov, volovich, hep-th, 9912242, ...   \n",
       "1004      [quantum, fields, responding, moving, mirrors,...   \n",
       "1005      [proceedings, lie, iii, clausthal, july, 1999,...   \n",
       "\n",
       "                                                  neighbors  \\\n",
       "paper_id                                                      \n",
       "1001      [9603161, 9702094, 9703082, 9507158, 9404151, ...   \n",
       "1002      [9912012, 9812035, 9902155, 9606076, 9809015, ...   \n",
       "1003      [205120, 9409090, 211182, 9912242, 208125, 970...   \n",
       "1004      [7129, 9305170, 9608190, 209232, 9910133, 1041...   \n",
       "1005      [9908025, 9909114, 4153, 9805026, 9307074, 960...   \n",
       "\n",
       "                                         abstract_embedding  \n",
       "paper_id                                                     \n",
       "1001      [0.016762745098039215, 0.15632156862745095, -0...  \n",
       "1002      [0.04356931818181819, 0.1502068181818182, -0.0...  \n",
       "1003      [-0.012898113207547173, 0.23114905660377363, -...  \n",
       "1004      [0.005588000000000004, 0.15716799999999997, 0....  \n",
       "1005      [0.08252173913043478, 0.15854347826086954, -0....  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_node_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new data for the node information\n",
    "# df_node_info.to_csv(\"data/node_information_completed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, the embeddings are stored in arrays, but when saving they are turned to strings !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node2vec embeddings have been created using this [implementation](https://github.com/snap-stanford/snap/tree/master/examples/node2vec) on our directed graph. When computing the embeddings for the edges, we have used the Hadamard product of the source and target 128-dimensional embeddings. If a node is isolated, we use a null vector as its embedding (cf `utils.nodes_to_edge_embedding`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27684 node embeddings\n"
     ]
    }
   ],
   "source": [
    "node_embeddings = {}\n",
    "with open(\"data/nodes.emb\", \"r\") as f:\n",
    "    next(f)\n",
    "    for i, line in enumerate(f):\n",
    "        node_id, vec = line.split(\" \", 1)\n",
    "        node_embeddings[node_id] = np.fromstring(vec, sep=\" \")\n",
    "    print(\"Loaded {} node embeddings\".format(len(node_embeddings)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 27770 nodes in total in the network, 86 nodes are isolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to create a new feature, simply add a method to `FeatureEngineering` (e.g. `jaccard_coefficient`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_node_info = pd.read_csv(\"data/node_information_completed.csv\", index_col=0)\n",
    "# # The nodes are represented as strings as in the network data\n",
    "df_node_info.index = df_node_info.index.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Opening the source-node data\n",
      "--> Computing and adding the edge embeddings\n",
      "--> Merging the source-node data with the node information\n",
      "--> Creating an undirected graph from the training data\n",
      "--> Creating a directed graph from the training data\n"
     ]
    }
   ],
   "source": [
    "feature_engineer = FeatureEngineering(df_node_info, node_embeddings,\n",
    "                                      path_embeddings=path_embeddings,\n",
    "                                      data=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to create \n",
    "features_to_create = [\"in_degree_source\", \"in_degree_target\", \"out_degree_source\", \"out_degree_target\", \n",
    "            \"title_overlap\", \"temp_diff\", \"abstract_overlap\", \n",
    "            \"is_same_journal\", \"jaccard_coefficient\", \"adamic_adar\", \"pref_attachment\", \n",
    "            \"common_neighbors\", \"abstract_cosine_similarity\"]\n",
    "embedding_columns = [\"emb{}\".format(i+1) for i in range(128)]\n",
    "\n",
    "features_columns = features_to_create + embedding_columns\n",
    "features_label_columns = features_columns + [\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Creating the feature 'in_degree_source' ...\n",
      "it took 0min 31s\n",
      "--> Creating the feature 'in_degree_target' ...\n",
      "it took 0min 23s\n",
      "--> Creating the feature 'out_degree_source' ...\n",
      "it took 0min 21s\n",
      "--> Creating the feature 'out_degree_target' ...\n",
      "it took 0min 21s\n",
      "--> Creating the feature 'title_overlap' ...\n",
      "it took 0min 31s\n",
      "--> Creating the feature 'temp_diff' ...\n",
      "it took 0min 27s\n",
      "--> Creating the feature 'abstract_overlap' ...\n",
      "it took 1min 19s\n",
      "--> Creating the feature 'is_same_journal' ...\n",
      "it took 0min 0s\n",
      "--> Creating the feature 'jaccard_coefficient' ...\n",
      "it took 0min 35s\n",
      "--> Creating the feature 'adamic_adar' ...\n",
      "it took 2min 22s\n",
      "--> Creating the feature 'pref_attachment' ...\n",
      "it took 0min 52s\n",
      "--> Creating the feature 'common_neighbors' ...\n",
      "it took 0min 34s\n",
      "--> Creating the feature 'abstract_cosine_similarity' ...\n",
      "Loaded 200000 pretrained word vectors\n",
      "it took 1min 43s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%time feature_engineer.create_features(features=features_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the created features \n",
    "feature_engineer.df[features_label_columns].to_csv(\"data/data_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils import random_forest_features_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we study the impact of the different features on the `f1-score`, using a Random Forest classifier with 10 trees.\n",
    "The importance of each feature corresponds to the \"gini importance\" or \"mean decrease impurity\" and is defined as the total decrease in node impurity (weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node) averaged over all trees of the ensemble.\n",
    "\n",
    "Let's denote by $F$ the set of **13 features**: `in_degree_source`, `in_degree_target`, `out_degree_source`, `out_degree_target`, `title_overlap`, `temp_diff`, `abstract_overlap`, `is_same_journal`, `jaccard_coefficient`, `adamic_adar`, `pref_attachment`, `common_neighbors`, `abstract_cosine_similarity`.\n",
    "\n",
    "We will compare the performance of the Random Forest on the *node2vec* 128 embeddings, the set $F$, and the two combined. This is done using a 5-fold cross-validation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_degree_source</th>\n",
       "      <th>in_degree_target</th>\n",
       "      <th>out_degree_source</th>\n",
       "      <th>out_degree_target</th>\n",
       "      <th>title_overlap</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>abstract_overlap</th>\n",
       "      <th>is_same_journal</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>adamic_adar</th>\n",
       "      <th>pref_attachment</th>\n",
       "      <th>common_neighbors</th>\n",
       "      <th>abstract_cosine_similarity</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>emb9</th>\n",
       "      <th>emb10</th>\n",
       "      <th>emb11</th>\n",
       "      <th>emb12</th>\n",
       "      <th>emb13</th>\n",
       "      <th>emb14</th>\n",
       "      <th>emb15</th>\n",
       "      <th>emb16</th>\n",
       "      <th>emb17</th>\n",
       "      <th>emb18</th>\n",
       "      <th>emb19</th>\n",
       "      <th>emb20</th>\n",
       "      <th>emb21</th>\n",
       "      <th>emb22</th>\n",
       "      <th>emb23</th>\n",
       "      <th>emb24</th>\n",
       "      <th>emb25</th>\n",
       "      <th>emb26</th>\n",
       "      <th>emb27</th>\n",
       "      <th>emb28</th>\n",
       "      <th>emb29</th>\n",
       "      <th>emb30</th>\n",
       "      <th>emb31</th>\n",
       "      <th>emb32</th>\n",
       "      <th>emb33</th>\n",
       "      <th>emb34</th>\n",
       "      <th>emb35</th>\n",
       "      <th>emb36</th>\n",
       "      <th>emb37</th>\n",
       "      <th>emb38</th>\n",
       "      <th>emb39</th>\n",
       "      <th>emb40</th>\n",
       "      <th>emb41</th>\n",
       "      <th>emb42</th>\n",
       "      <th>emb43</th>\n",
       "      <th>emb44</th>\n",
       "      <th>emb45</th>\n",
       "      <th>emb46</th>\n",
       "      <th>emb47</th>\n",
       "      <th>emb48</th>\n",
       "      <th>emb49</th>\n",
       "      <th>emb50</th>\n",
       "      <th>emb51</th>\n",
       "      <th>emb52</th>\n",
       "      <th>emb53</th>\n",
       "      <th>emb54</th>\n",
       "      <th>emb55</th>\n",
       "      <th>emb56</th>\n",
       "      <th>emb57</th>\n",
       "      <th>emb58</th>\n",
       "      <th>emb59</th>\n",
       "      <th>emb60</th>\n",
       "      <th>emb61</th>\n",
       "      <th>emb62</th>\n",
       "      <th>emb63</th>\n",
       "      <th>emb64</th>\n",
       "      <th>emb65</th>\n",
       "      <th>emb66</th>\n",
       "      <th>emb67</th>\n",
       "      <th>emb68</th>\n",
       "      <th>emb69</th>\n",
       "      <th>emb70</th>\n",
       "      <th>emb71</th>\n",
       "      <th>emb72</th>\n",
       "      <th>emb73</th>\n",
       "      <th>emb74</th>\n",
       "      <th>emb75</th>\n",
       "      <th>emb76</th>\n",
       "      <th>emb77</th>\n",
       "      <th>emb78</th>\n",
       "      <th>emb79</th>\n",
       "      <th>emb80</th>\n",
       "      <th>emb81</th>\n",
       "      <th>emb82</th>\n",
       "      <th>emb83</th>\n",
       "      <th>emb84</th>\n",
       "      <th>emb85</th>\n",
       "      <th>emb86</th>\n",
       "      <th>emb87</th>\n",
       "      <th>emb88</th>\n",
       "      <th>emb89</th>\n",
       "      <th>emb90</th>\n",
       "      <th>emb91</th>\n",
       "      <th>emb92</th>\n",
       "      <th>emb93</th>\n",
       "      <th>emb94</th>\n",
       "      <th>emb95</th>\n",
       "      <th>emb96</th>\n",
       "      <th>emb97</th>\n",
       "      <th>emb98</th>\n",
       "      <th>emb99</th>\n",
       "      <th>emb100</th>\n",
       "      <th>emb101</th>\n",
       "      <th>emb102</th>\n",
       "      <th>emb103</th>\n",
       "      <th>emb104</th>\n",
       "      <th>emb105</th>\n",
       "      <th>emb106</th>\n",
       "      <th>emb107</th>\n",
       "      <th>emb108</th>\n",
       "      <th>emb109</th>\n",
       "      <th>emb110</th>\n",
       "      <th>emb111</th>\n",
       "      <th>emb112</th>\n",
       "      <th>emb113</th>\n",
       "      <th>emb114</th>\n",
       "      <th>emb115</th>\n",
       "      <th>emb116</th>\n",
       "      <th>emb117</th>\n",
       "      <th>emb118</th>\n",
       "      <th>emb119</th>\n",
       "      <th>emb120</th>\n",
       "      <th>emb121</th>\n",
       "      <th>emb122</th>\n",
       "      <th>emb123</th>\n",
       "      <th>emb124</th>\n",
       "      <th>emb125</th>\n",
       "      <th>emb126</th>\n",
       "      <th>emb127</th>\n",
       "      <th>emb128</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.513898</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.014731</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>5.352388e-04</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>1.830387e-04</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.014427</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.004332</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>5.923105e-04</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.006299</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>7.004193e-04</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.014016</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>-3.068602e-06</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.010570</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.694162e-04</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>7.196812e-08</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>-9.162506e-07</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>124</td>\n",
       "      <td>68</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>4.320366</td>\n",
       "      <td>11613</td>\n",
       "      <td>8</td>\n",
       "      <td>0.761980</td>\n",
       "      <td>0.123033</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.020730</td>\n",
       "      <td>0.023512</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>0.019526</td>\n",
       "      <td>-0.004014</td>\n",
       "      <td>-6.028826e-04</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>-0.001530</td>\n",
       "      <td>1.957685e-02</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.034494</td>\n",
       "      <td>0.029158</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.027425</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.034871</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>0.033106</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.033401</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.026819</td>\n",
       "      <td>0.028680</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.072009</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>0.007942</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>0.043186</td>\n",
       "      <td>0.020565</td>\n",
       "      <td>0.020066</td>\n",
       "      <td>0.044115</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>7.623107e-03</td>\n",
       "      <td>0.014223</td>\n",
       "      <td>0.042206</td>\n",
       "      <td>0.037443</td>\n",
       "      <td>0.051150</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.053607</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.037578</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.095798</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.045593</td>\n",
       "      <td>0.061407</td>\n",
       "      <td>3.641000e-02</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.059116</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.029389</td>\n",
       "      <td>0.060083</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>8.496925e-03</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.044984</td>\n",
       "      <td>0.106443</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.025649</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>0.025237</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.010391</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>7.186248e-03</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>-0.002430</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.017884</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.012603</td>\n",
       "      <td>0.048350</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.174836</td>\n",
       "      <td>0.036171</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>-2.759207e-04</td>\n",
       "      <td>0.020012</td>\n",
       "      <td>-0.002992</td>\n",
       "      <td>0.022618</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>1.383531e-02</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.067939</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877725</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-6.192968e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>4.240570e-07</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2.125438e-05</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>2.899191e-07</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>-2.721497e-05</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-4.733297e-05</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-1.718414e-05</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-8.183345e-06</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879783</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>2.119637e-04</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>3.246911e-03</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.012217</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-5.788020e-07</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>2.172159e-03</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>8.062960e-07</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.007610</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.008946</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>8.076983e-07</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>-3.524962e-05</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>6.407609e-04</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867293</td>\n",
       "      <td>0.093691</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>-0.003602</td>\n",
       "      <td>0.065103</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>-0.001295</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>7.413848e-04</td>\n",
       "      <td>0.008477</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>4.601420e-04</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.030279</td>\n",
       "      <td>0.026353</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.070054</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>-0.005761</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>0.104011</td>\n",
       "      <td>-0.005871</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.020263</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>-0.004372</td>\n",
       "      <td>-0.002243</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-3.573135e-04</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.007205</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.018538</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>0.014882</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>0.047959</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>-0.017720</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>-0.005898</td>\n",
       "      <td>-0.005054</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>4.438680e-03</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>0.049572</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.080927</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>-3.785353e-03</td>\n",
       "      <td>-0.000824</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>-0.000473</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.047012</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>-0.001273</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>-0.005336</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.015874</td>\n",
       "      <td>-0.007837</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.041646</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>-0.000870</td>\n",
       "      <td>0.023891</td>\n",
       "      <td>0.012123</td>\n",
       "      <td>3.281416e-03</td>\n",
       "      <td>0.027422</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.059960</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>0.025839</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>3.025043e-04</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>0.020399</td>\n",
       "      <td>-0.007542</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>-3.753545e-03</td>\n",
       "      <td>0.030252</td>\n",
       "      <td>0.071255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   in_degree_source  in_degree_target  out_degree_source  out_degree_target  \\\n",
       "0                 3                 8                  3                  4   \n",
       "1                11               124                 68                 23   \n",
       "2                 1                 2                  0                  3   \n",
       "3                 4                 2                 16                 12   \n",
       "4                 7                 2                  0                 22   \n",
       "\n",
       "   title_overlap  temp_diff  abstract_overlap  is_same_journal  \\\n",
       "0              2          0                 6                1   \n",
       "1              0          1                 6                0   \n",
       "2              0         -2                 4                0   \n",
       "3              0         -4                 6                0   \n",
       "4              0         -5                 5                0   \n",
       "\n",
       "   jaccard_coefficient  adamic_adar  pref_attachment  common_neighbors  \\\n",
       "0             0.062500     0.513898               72                 1   \n",
       "1             0.069565     4.320366            11613                 8   \n",
       "2             0.000000     0.000000                5                 0   \n",
       "3             0.000000     0.000000              280                 0   \n",
       "4             0.000000     0.000000              168                 0   \n",
       "\n",
       "   abstract_cosine_similarity      emb1      emb2      emb3      emb4  \\\n",
       "0                    0.819155  0.014731  0.000464  0.001126  0.006963   \n",
       "1                    0.761980  0.123033  0.007381  0.020730  0.023512   \n",
       "2                    0.877725  0.000013 -0.000027  0.000011  0.000405   \n",
       "3                    0.879783  0.014054  0.003098  0.000073  0.010046   \n",
       "4                    0.867293  0.093691  0.009185 -0.003602  0.065103   \n",
       "\n",
       "       emb5      emb6      emb7      emb8          emb9     emb10     emb11  \\\n",
       "0 -0.000007 -0.000003  0.001664  0.001189  5.352388e-04  0.001202  0.003600   \n",
       "1  0.000234  0.009455  0.019526 -0.004014 -6.028826e-04  0.001052  0.004027   \n",
       "2 -0.000029  0.000043  0.000004  0.000022 -6.192968e-08  0.000002  0.000019   \n",
       "3  0.001085 -0.000076  0.004628  0.001957  2.119637e-04  0.000323 -0.000200   \n",
       "4  0.001643 -0.001295  0.017832  0.003726  7.413848e-04  0.008477  0.017117   \n",
       "\n",
       "      emb12     emb13     emb14     emb15         emb16     emb17     emb18  \\\n",
       "0  0.001908  0.002489  0.000068  0.002816  1.830387e-04  0.004138  0.005182   \n",
       "1  0.000473  0.021602  0.002667 -0.001530  1.957685e-02  0.003978  0.034494   \n",
       "2  0.000135  0.000206 -0.000002  0.000298  4.240570e-07  0.000569  0.000242   \n",
       "3 -0.000107  0.008225  0.000725  0.001116  3.246911e-03  0.000691  0.006976   \n",
       "4  0.001971  0.007056  0.000137  0.008885  4.601420e-04  0.029260  0.030279   \n",
       "\n",
       "      emb19     emb20     emb21     emb22     emb23     emb24     emb25  \\\n",
       "0  0.004700  0.001164  0.000011  0.013604  0.000652  0.001165  0.001034   \n",
       "1  0.029158  0.000161  0.009475  0.027425 -0.001157  0.000342  0.003899   \n",
       "2 -0.000173  0.000006  0.000024  0.000893  0.000020  0.000028 -0.000025   \n",
       "3  0.001837 -0.000329  0.000539  0.002002  0.002390 -0.000002  0.000141   \n",
       "4  0.026353  0.002655 -0.000106  0.070054  0.001016  0.010414  0.000194   \n",
       "\n",
       "      emb26     emb27     emb28     emb29     emb30     emb31     emb32  \\\n",
       "0  0.000507  0.000970  0.004729  0.000006  0.014427  0.001276  0.003345   \n",
       "1  0.012318  0.002769  0.034871  0.016022  0.014213  0.015737  0.033106   \n",
       "2  0.000045  0.000131  0.000087  0.000001  0.000051 -0.000078 -0.000015   \n",
       "3  0.000051 -0.000294  0.001427  0.000757  0.012217  0.000524  0.005000   \n",
       "4 -0.005761 -0.000546  0.004408 -0.001850  0.104011 -0.005871  0.012728   \n",
       "\n",
       "      emb33     emb34     emb35     emb36     emb37     emb38     emb39  \\\n",
       "0  0.001294  0.001742  0.000058  0.004332  0.003503  0.003274  0.003499   \n",
       "1  0.002641  0.033401  0.001663  0.026819  0.028680  0.007182  0.072009   \n",
       "2  0.000153 -0.000100  0.000065  0.000487  0.000115  0.000232  0.000149   \n",
       "3 -0.000230  0.003570  0.000979  0.006869  0.002421 -0.000156  0.008894   \n",
       "4  0.001945  0.020263 -0.000192  0.027477  0.012924  0.017392  0.028633   \n",
       "\n",
       "      emb40     emb41     emb42     emb43     emb44     emb45     emb46  \\\n",
       "0  0.001227  0.000033  0.006026  0.000693  0.000048  0.001444  0.000606   \n",
       "1 -0.002329  0.007942 -0.001540  0.043186  0.020565  0.020066  0.044115   \n",
       "2  0.000029 -0.000017  0.000358  0.000005  0.000001 -0.000168 -0.000050   \n",
       "3  0.001075  0.000061  0.002969  0.002843  0.001542  0.001378  0.003581   \n",
       "4  0.008057 -0.003636  0.017838 -0.004372 -0.002243  0.008336  0.006933   \n",
       "\n",
       "      emb47     emb48     emb49         emb50     emb51     emb52     emb53  \\\n",
       "0  0.002730  0.000328  0.000079  5.923105e-04  0.000210 -0.000035  0.000550   \n",
       "1  0.023695  0.005646  0.001308  7.623107e-03  0.014223  0.042206  0.037443   \n",
       "2  0.000305  0.000063  0.000013  2.125438e-05 -0.000014 -0.000006 -0.000064   \n",
       "3  0.001499  0.000620 -0.000057 -5.788020e-07  0.001670  0.001619  0.003595   \n",
       "4  0.002636  0.000810 -0.000008 -3.573135e-04 -0.002556 -0.007205  0.007523   \n",
       "\n",
       "      emb54     emb55     emb56     emb57     emb58     emb59     emb60  \\\n",
       "0  0.001777  0.001086  0.002278  0.000407  0.006299  0.002975  0.001543   \n",
       "1  0.051150  0.000084  0.000226  0.005331  0.008431  0.053607  0.003158   \n",
       "2  0.000041  0.000031  0.000355 -0.000033 -0.000011 -0.000015 -0.000303   \n",
       "3  0.001207  0.000085  0.001313  0.000049  0.002021  0.004743  0.000711   \n",
       "4  0.000147  0.000081  0.018538  0.000723  0.022655  0.014882  0.013870   \n",
       "\n",
       "      emb61     emb62     emb63     emb64     emb65     emb66     emb67  \\\n",
       "0  0.008894  0.000045  0.000500  0.000825  0.000451  0.000677  0.003411   \n",
       "1  0.037578  0.002302  0.095798  0.004114  0.006717  0.045593  0.061407   \n",
       "2  0.000381 -0.000009  0.000003  0.000166  0.000057 -0.000064 -0.000064   \n",
       "3  0.010932  0.000194  0.012072 -0.000091  0.000389  0.000885  0.002043   \n",
       "4  0.047959 -0.000182 -0.017720  0.002794 -0.005898 -0.005054  0.004799   \n",
       "\n",
       "          emb68     emb69     emb70     emb71     emb72     emb73     emb74  \\\n",
       "0  7.004193e-04  0.000093  0.010883  0.000975  0.000548  0.014016  0.001179   \n",
       "1  3.641000e-02  0.001512  0.059116  0.024058  0.004114  0.029389  0.060083   \n",
       "2  2.899191e-07  0.000017  0.000414 -0.000017  0.000241  0.001270 -0.000104   \n",
       "3  2.172159e-03 -0.000013  0.005111  0.001297  0.000003  0.006868  0.007009   \n",
       "4  4.438680e-03 -0.001133  0.049572  0.003903  0.004652  0.080927 -0.002528   \n",
       "\n",
       "      emb75     emb76         emb77     emb78     emb79     emb80     emb81  \\\n",
       "0  0.001621  0.005963 -3.068602e-06  0.000104  0.000155  0.001787  0.001740   \n",
       "1  0.000558  0.026799  8.496925e-03  0.002291  0.000250  0.027323  0.001640   \n",
       "2  0.000002  0.000210 -2.721497e-05 -0.000073  0.000004  0.000084 -0.000181   \n",
       "3  0.000785  0.004941  8.062960e-07  0.000134  0.000053  0.000385 -0.000016   \n",
       "4  0.007795  0.036761 -3.785353e-03 -0.000824  0.000291 -0.000473  0.005611   \n",
       "\n",
       "      emb82     emb83     emb84     emb85     emb86     emb87     emb88  \\\n",
       "0  0.001554  0.000305  0.000535  0.002769  0.000003  0.010570  0.001702   \n",
       "1  0.007626  0.002708  0.044984  0.106443  0.000394  0.025649  0.019979   \n",
       "2 -0.000072 -0.000016 -0.000058  0.000070 -0.000003  0.000683 -0.000047   \n",
       "3 -0.000038  0.003038  0.003636  0.009585  0.000395  0.002335  0.007610   \n",
       "4  0.008121  0.000068  0.007100  0.013329  0.002401  0.047012  0.007536   \n",
       "\n",
       "      emb89     emb90     emb91     emb92     emb93     emb94     emb95  \\\n",
       "0  0.000733  0.003305  0.000006  0.008535  0.000389  0.002724  0.001640   \n",
       "1  0.006718  0.025237  0.008532  0.010970  0.010674  0.019662  0.007864   \n",
       "2  0.000076  0.000212  0.000021  0.000524 -0.000001  0.000436 -0.000157   \n",
       "3  0.000521  0.000371  0.000405  0.005273  0.000062  0.000020  0.000389   \n",
       "4  0.004586  0.016528 -0.001273  0.054956  0.000077 -0.005336  0.005482   \n",
       "\n",
       "      emb96     emb97     emb98     emb99    emb100    emb101    emb102  \\\n",
       "0  0.001407  0.000012  0.001414  0.006466  0.004306  0.002465  0.000341   \n",
       "1  0.019365  0.028534  0.020503  0.041992  0.003757  0.003275  0.010391   \n",
       "2 -0.000165  0.000036 -0.000003  0.000351 -0.000006 -0.000153 -0.000020   \n",
       "3  0.000123  0.000054  0.008946  0.018382  0.001181 -0.000153  0.000007   \n",
       "4  0.015874 -0.007837  0.004038  0.041646  0.013144  0.010188 -0.000870   \n",
       "\n",
       "     emb103    emb104        emb105    emb106    emb107    emb108    emb109  \\\n",
       "0  0.003546  0.000154  1.694162e-04  0.002555  0.000421  0.000222  0.000870   \n",
       "1  0.007431  0.021888  7.186248e-03  0.018724 -0.002430  0.019554  0.017884   \n",
       "2  0.000351 -0.000003 -4.733297e-05  0.000083 -0.000059  0.000042  0.000002   \n",
       "3 -0.000037  0.004846  8.076983e-07  0.003720  0.000115  0.000101  0.000414   \n",
       "4  0.023891  0.012123  3.281416e-03  0.027422  0.001005  0.006887  0.000297   \n",
       "\n",
       "     emb110    emb111    emb112    emb113    emb114    emb115    emb116  \\\n",
       "0  0.000006  0.000186  0.000324  0.001615  0.003179 -0.000002  0.008223   \n",
       "1  0.001593  0.001542  0.000282  0.012603  0.048350  0.003189  0.174836   \n",
       "2  0.000021  0.000006  0.000029 -0.000235  0.000068 -0.000019 -0.000058   \n",
       "3  0.000032 -0.000042 -0.000003  0.000967  0.003987  0.002552  0.015432   \n",
       "4  0.001981 -0.000317 -0.000297  0.012318  0.012286  0.000815  0.059960   \n",
       "\n",
       "     emb117    emb118    emb119    emb120        emb121    emb122    emb123  \\\n",
       "0  0.000525  0.003989  0.000791  0.000270  7.196812e-08  0.000015  0.004535   \n",
       "1  0.036171  0.000395  0.036458  0.009747 -2.759207e-04  0.020012 -0.002992   \n",
       "2  0.000002  0.000112 -0.000107  0.000004 -1.718414e-05  0.000003  0.000422   \n",
       "3  0.001204  0.001127  0.002007  0.000433 -3.524962e-05  0.000032  0.004378   \n",
       "4 -0.003851  0.025839 -0.009964 -0.000852  3.025043e-04 -0.001324  0.020399   \n",
       "\n",
       "     emb124    emb125        emb126    emb127    emb128  label  \n",
       "0  0.000520  0.000669 -9.162506e-07  0.005291  0.009761      1  \n",
       "1  0.022618  0.005407  1.383531e-02  0.000693  0.067939      1  \n",
       "2  0.000080  0.000102 -8.183345e-06  0.000391  0.000612      0  \n",
       "3  0.000748  0.000742  6.407609e-04  0.000203  0.015800      0  \n",
       "4 -0.007542  0.006621 -3.753545e-03  0.030252  0.071255      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615512, 142)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings columns\n",
    "node2vec_features = [col for col in df.columns if \"emb\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 features are used\n"
     ]
    }
   ],
   "source": [
    "X = df[node2vec_features]\n",
    "y = df.label\n",
    "print(\"{} features are used\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   59.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.832\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Set a 5-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.3f}\"\\\n",
    "    .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set of features $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_features = [\"in_degree_source\", \"in_degree_target\", \"out_degree_source\", \n",
    "              \"out_degree_target\", \"title_overlap\", \"temp_diff\", \"abstract_overlap\", \n",
    "              \"is_same_journal\", \"jaccard_coefficient\", \"adamic_adar\", \"pref_attachment\", \n",
    "              \"common_neighbors\", \"abstract_cosine_similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 features are used\n"
     ]
    }
   ],
   "source": [
    "X = df[F_features]\n",
    "y = df.label\n",
    "print(\"{} features are used\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.971\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Set a 5-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.3f}\"\\\n",
    "    .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature adamic_adar (0.385508)\n",
      "2. feature common_neighbors (0.109787)\n",
      "3. feature pref_attachment (0.105008)\n",
      "4. feature abstract_overlap (0.091937)\n",
      "5. feature jaccard_coefficient (0.084497)\n",
      "6. feature temp_diff (0.070305)\n",
      "7. feature in_degree_target (0.059783)\n",
      "8. feature abstract_cosine_similarity (0.028205)\n",
      "9. feature title_overlap (0.018390)\n",
      "10. feature out_degree_target (0.016828)\n",
      "11. feature out_degree_source (0.016638)\n",
      "12. feature in_degree_source (0.011752)\n",
      "13. feature is_same_journal (0.001363)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA60AAAJzCAYAAADk5vU0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkrQtGbQmSZIkSZIkSZIkbcv/A78jILSzIN3mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25d67d07ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the importances of the features used\n",
    "top_features = random_forest_features_importances(X, y, n_estimators=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of node2vec and features $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 features are used\n"
     ]
    }
   ],
   "source": [
    "X = df[F_features + node2vec_features]\n",
    "y = df.label\n",
    "print(\"{} features are used\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   50.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   55.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   54.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   60.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   59.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.970\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Set a 5-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.3f}\"\\\n",
    "    .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Features used            | Mean cross-validated f1-score |\n",
    "|--------------------------|-------------------------------|\n",
    "| node2vec 128 embeddings  | 0.832                         |\n",
    "| $F$ features             | 0.971                         |\n",
    "|  node2vec + $F$          | 0.970                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To summarize, the use of the set of features $F$ gives the best results. Using node2vec features and the set $F$ gives similar results but the number of features is more than 10 times higher. Thus, we will use the set of features $F$ to compare different algorithms in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the set $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following, we try to select the K best\n",
    "# features by plotting the score on 3-fold cross-val\n",
    "# as a function of K\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=10)\n",
    "X = df[F_features]\n",
    "y = df.label\n",
    "\n",
    "scores = []\n",
    "for i in range(len(top_features)):\n",
    "    features_tested = top_features[:i+1]\n",
    "    X_tested = X[features_tested]\n",
    "    score = cross_val_score(clf, X_tested, y, cv=3, scoring='f1').mean()\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEaCAYAAADZvco2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFRERCQz/x8Rob7zcX49lQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25d661cada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,len(top_features)+1), scores)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"Score = f(K best features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = [\"in_degree_target\", \"out_degree_target\", \"title_overlap\", \n",
    "                 \"temp_diff\", \"abstract_overlap\", \"jaccard_coefficient\", \n",
    "                 \"adamic_adar\", \"pref_attachment\", \"common_neighbors\", \n",
    "                 \"abstract_cosine_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compare multiple classifiers using the set of features $F$. We will use the default `scikit-learn` parameters for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 features are used\n"
     ]
    }
   ],
   "source": [
    "X = df[best_features]\n",
    "y = df.label\n",
    "print(\"{} features are used\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a 5-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   20.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.944\n"
     ]
    }
   ],
   "source": [
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.3f}\"\\\n",
    "    .format(cross_val_score(lr, X, y, cv=cv, scoring=\"f1\", n_jobs=-1, verbose=2)\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   52.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.970\n"
     ]
    }
   ],
   "source": [
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.3f}\"\\\n",
    "    .format(cross_val_score(rf, X, y, cv=cv, scoring=\"f1\", n_jobs=-1, verbose=2)\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.971\n"
     ]
    }
   ],
   "source": [
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.3f}\"\\\n",
    "    .format(cross_val_score(gb, X, y, cv=cv, scoring=\"f1\", n_jobs=-1, verbose=2)\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm                | Mean cross-validated f1-score |\n",
    "|--------------------------|-------------------------------|\n",
    "| Logistic Regression      | 0.944                         |\n",
    "| Random Forest Classifier | 0.970                         |\n",
    "| Gradient Boosting        | 0.971                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will choose the Gradient Boosting classifier given these comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting hyperparameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computations being too slow in the notebook, we have used the script `gradient_boosting_optimization.py` in order to tune the different parameters of the Gradient Boosting classifier. We have optimized the following parameters in this order using :\n",
    "\n",
    "1.  `n_estimators`\n",
    "2. `max_depth` and `min_samples_split`\n",
    "3. `min_samples_leaf`\n",
    "\n",
    "The results obtained are in the files `gradient_boosting_hyperparameters_n.txt` where n is the order the tuning.\n",
    "\n",
    "We have chosen 50 `estimators` since it gave already good results, and increasing the number of estimators would slow down the learning for the other parameters tunings. For the `max_depth`, we have chosen 7 since it gave similar results to a max depth of 9 (which has the disadvantage of overfitting more). Finally, these paramaters, combined with `min_samples_split` = 200 and `min_samples_leaf` = 60 gave the best results in terms of the 5-fold cross-validated mean f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import complete_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Opening the source-node data\n",
      "--> Computing and adding the edge embeddings\n",
      "--> Merging the source-node data with the node information\n",
      "--> Creating an undirected graph from the training data\n",
      "--> Creating a directed graph from the training data\n"
     ]
    }
   ],
   "source": [
    "feature_engineer_test = FeatureEngineering(df_node_info, \n",
    "                                           data=\"test\",\n",
    "                                           path_word_embeddings=path_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Creating the feature 'in_degree_target' ...\n",
      "it took 0min 1s\n",
      "--> Creating the feature 'out_degree_target' ...\n",
      "it took 0min 1s\n",
      "--> Creating the feature 'title_overlap' ...\n",
      "it took 0min 2s\n",
      "--> Creating the feature 'temp_diff' ...\n",
      "it took 0min 1s\n",
      "--> Creating the feature 'abstract_overlap' ...\n",
      "it took 0min 4s\n",
      "--> Creating the feature 'jaccard_coefficient' ...\n",
      "it took 0min 2s\n",
      "--> Creating the feature 'adamic_adar' ...\n",
      "it took 0min 9s\n",
      "--> Creating the feature 'pref_attachment' ...\n",
      "it took 0min 3s\n",
      "--> Creating the feature 'common_neighbors' ...\n",
      "it took 0min 2s\n",
      "--> Creating the feature 'abstract_cosine_similarity' ...\n",
      "Loaded 200000 pretrained word vectors\n",
      "it took 0min 50s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%time feature_engineer_test.create_features(features=best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data \n",
    "X_test = feature_engineer_test.df[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2089            3.83m\n",
      "         2           1.0703            4.44m\n",
      "         3           0.9551            4.34m\n",
      "         4           0.8579            4.38m\n",
      "         5           0.7750            4.22m\n",
      "         6           0.7039            3.94m\n",
      "         7           0.6425            3.82m\n",
      "         8           0.5891            3.64m\n",
      "         9           0.5426            3.46m\n",
      "        10           0.5017            3.31m\n",
      "        20           0.2810            2.33m\n",
      "        30           0.2101            1.53m\n",
      "        40           0.1851           46.68s\n",
      "        50           0.1753            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=7,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=60, min_samples_split=200,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=1,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=50, min_samples_split=200,\n",
    "                                 min_samples_leaf=60, max_depth=7, verbose=1)\n",
    "clf.fit(X[best_features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = list(clf.predict(X_test[X_test.temp_diff >= 0]))\n",
    "predictions = complete_predictions(X_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name for the predictions\n",
    "file_name_prefix = \"GB_final\"\n",
    "file_name = file_name_prefix + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category\n",
       "id          \n",
       "0          0\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.DataFrame(predictions, columns=[\"category\"])\n",
    "df_predictions.index.names = [\"id\"]\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "df_predictions.to_csv(\"predictions/{}\".format(file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
