{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "#import jgraph\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import copy as cp\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from datetime import date\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict missing links in citation networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to identify missing citations in a citation network of research articles. A citation network is represented as a graph G=(V, E), where the nodes correspond to scientific articles and the existence of a directed edge between nodes u and v, indicates that paper u cites paper v.\n",
    "\n",
    "Each node (i.e., article) is also associated with information such as the title of the paper, publication year, author names and a short abstract. A number of edges have been randomly removed from the original citation network.\n",
    "\n",
    "Your goal is to accurately reconstruct the initial network using graph-theoretical and textual features, and possibly other information. Your solution can be based on supervised or unsupervised techniques for link prediction or on a combination of both. You should aim for the maximum F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "with open(\"data/testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "df_test = pd.DataFrame(testing_set, columns=[\"source\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "with open(\"data/training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "df_train = pd.DataFrame(training_set, columns=[\"source\", \"target\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615512, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source   target label\n",
       "0  9510123  9502114     1\n",
       "1  9707075  9604178     1\n",
       "2  9312155  9506142     0\n",
       "3  9911255   302165     0\n",
       "4  9701033   209076     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"label\"] = df_train.label.map(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source   target  label\n",
       "0  9510123  9502114      1\n",
       "1  9707075  9604178      1\n",
       "2  9312155  9506142      0\n",
       "3  9911255   302165      0\n",
       "4  9701033   209076      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct neighbors for each source node\n",
    "df_neighbors = df_train.groupby(\"source\")[\"target\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for each node\n",
    "with open(\"data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "df_node_info = pd.DataFrame(node_info, columns=[\"paper_id\", \"year\", \"title\", \n",
    "                                                \"authors\", \"journal_name\", \"abstract\"])\n",
    "df_node_info.index = df_node_info.paper_id\n",
    "df_node_info.drop(\"paper_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the year column to numeric setting\n",
    "df_node_info[\"year\"] = df_node_info.year.map(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(t):\n",
    "    # Remove stopwords, lower and tokenize\n",
    "    result = t.lower().split(\" \")\n",
    "    result = [token for token in result if token not in stpwds]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info[\"prcssd_title\"] = df_node_info[\"title\"].map(lambda t: process_text(t))\n",
    "df_node_info[\"prcssd_authors\"] = df_node_info[\"authors\"].map(lambda l: [author for author in l.split(\",\") if author not in (\" \", \"\")])\n",
    "df_node_info[\"prcssd_abstract\"] = df_node_info[\"abstract\"].map(lambda t: process_text(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the author names to be consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_author_name(author_name):\n",
    "    \"\"\"Format the author name (string) as N Benabderrazik. If there are \n",
    "    multiple first names it would be: A B FamilyName where A and B are \n",
    "    the first letters of the respective first names.\n",
    "    \"\"\"\n",
    "    # Keep only strings before an opening parenthesis (e.g. discard (Montpellier))\n",
    "    author_name = re.sub(r\"\\(.*$\", \"\", author_name)\n",
    "    \n",
    "    # Keep only word characters\n",
    "    author_name = re.findall(r\"[\\w']+\", author_name)\n",
    "    \n",
    "    # Turn the first names to initials and keep the family name as is\n",
    "    author_name = [author_name[i][0] if i < len(author_name)-1 else author_name[i] \n",
    "              for i in range(len(author_name))]\n",
    "    \n",
    "    return \" \".join(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_author_names(author_names):\n",
    "    \"\"\"Format a list of author names.\n",
    "    \"\"\"\n",
    "    return [format_author_name(author_name) for author_name in author_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info[\"prcssd_authors\"] = df_node_info[\"prcssd_authors\"]\\\n",
    "                                  .apply(lambda x: format_author_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>prcssd_title</th>\n",
       "      <th>prcssd_authors</th>\n",
       "      <th>prcssd_abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[P S Aspinwall]</td>\n",
       "      <td>[notes, based, lectures, given, tasi99, review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, walls, massive, gauged, supergravity,...</td>\n",
       "      <td>[M Cvetic, H Lu, C N Pope]</td>\n",
       "      <td>[point, massive, gauged, supergravity, potenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuations, brane, worlds]</td>\n",
       "      <td>[Y S Myung, G Kang]</td>\n",
       "      <td>[recently, ivanov, volovich, hep-th, 9912242, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[moving, mirrors, thermodynamic, paradoxes]</td>\n",
       "      <td>[A D Helfer]</td>\n",
       "      <td>[quantum, fields, responding, moving, mirrors,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundles, chiral, blocks, boundary, conditions...</td>\n",
       "      <td>[J Fuchs, C Schweigert]</td>\n",
       "      <td>[proceedings, lie, iii, clausthal, july, 1999,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year                                              title  \\\n",
       "paper_id                                                            \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                              authors       journal_name  \\\n",
       "paper_id                                                   \n",
       "1001                Paul S. Aspinwall                      \n",
       "1002      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "1003         Y.S. Myung, Gungwon Kang                      \n",
       "1004                   Adam D. Helfer          Phys.Rev.   \n",
       "1005          J. Fuchs, C. Schweigert                      \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "1001      these are notes based on lectures given at tas...   \n",
       "1002      we point out that massive gauged supergravity ...   \n",
       "1003      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "1004      quantum fields responding to moving mirrors ha...   \n",
       "1005      proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                                               prcssd_title  \\\n",
       "paper_id                                                      \n",
       "1001                  [compactification, geometry, duality]   \n",
       "1002      [domain, walls, massive, gauged, supergravity,...   \n",
       "1003         [comment, metric, fluctuations, brane, worlds]   \n",
       "1004            [moving, mirrors, thermodynamic, paradoxes]   \n",
       "1005      [bundles, chiral, blocks, boundary, conditions...   \n",
       "\n",
       "                      prcssd_authors  \\\n",
       "paper_id                               \n",
       "1001                 [P S Aspinwall]   \n",
       "1002      [M Cvetic, H Lu, C N Pope]   \n",
       "1003             [Y S Myung, G Kang]   \n",
       "1004                    [A D Helfer]   \n",
       "1005         [J Fuchs, C Schweigert]   \n",
       "\n",
       "                                            prcssd_abstract  \n",
       "paper_id                                                     \n",
       "1001      [notes, based, lectures, given, tasi99, review...  \n",
       "1002      [point, massive, gauged, supergravity, potenti...  \n",
       "1003      [recently, ivanov, volovich, hep-th, 9912242, ...  \n",
       "1004      [quantum, fields, responding, moving, mirrors,...  \n",
       "1005      [proceedings, lie, iii, clausthal, july, 1999,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_node_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we get the direct neighbors for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct neighbors for each source node\n",
    "df_neighbors = df_train.groupby(\"source\")[\"target\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info = df_node_info.join(df_neighbors)\n",
    "df_node_info.rename(columns={\"target\": \"neighbors\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>prcssd_title</th>\n",
       "      <th>prcssd_authors</th>\n",
       "      <th>prcssd_abstract</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[P S Aspinwall]</td>\n",
       "      <td>[notes, based, lectures, given, tasi99, review...</td>\n",
       "      <td>9603161 9702094 9703082 9507158 9404151 950103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, walls, massive, gauged, supergravity,...</td>\n",
       "      <td>[M Cvetic, H Lu, C N Pope]</td>\n",
       "      <td>[point, massive, gauged, supergravity, potenti...</td>\n",
       "      <td>9912012 9812035 9902155 9606076 9809015 4158 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuations, brane, worlds]</td>\n",
       "      <td>[Y S Myung, G Kang]</td>\n",
       "      <td>[recently, ivanov, volovich, hep-th, 9912242, ...</td>\n",
       "      <td>205120 9409090 211182 9912242 208125 9709049 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[moving, mirrors, thermodynamic, paradoxes]</td>\n",
       "      <td>[A D Helfer]</td>\n",
       "      <td>[quantum, fields, responding, moving, mirrors,...</td>\n",
       "      <td>7129 9305170 9608190 209232 9910133 104118 990...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundles, chiral, blocks, boundary, conditions...</td>\n",
       "      <td>[J Fuchs, C Schweigert]</td>\n",
       "      <td>[proceedings, lie, iii, clausthal, july, 1999,...</td>\n",
       "      <td>9908025 9909114 4153 9805026 9307074 9607157 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year                                              title  \\\n",
       "paper_id                                                            \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                              authors       journal_name  \\\n",
       "paper_id                                                   \n",
       "1001                Paul S. Aspinwall                      \n",
       "1002      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "1003         Y.S. Myung, Gungwon Kang                      \n",
       "1004                   Adam D. Helfer          Phys.Rev.   \n",
       "1005          J. Fuchs, C. Schweigert                      \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "1001      these are notes based on lectures given at tas...   \n",
       "1002      we point out that massive gauged supergravity ...   \n",
       "1003      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "1004      quantum fields responding to moving mirrors ha...   \n",
       "1005      proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                                               prcssd_title  \\\n",
       "paper_id                                                      \n",
       "1001                  [compactification, geometry, duality]   \n",
       "1002      [domain, walls, massive, gauged, supergravity,...   \n",
       "1003         [comment, metric, fluctuations, brane, worlds]   \n",
       "1004            [moving, mirrors, thermodynamic, paradoxes]   \n",
       "1005      [bundles, chiral, blocks, boundary, conditions...   \n",
       "\n",
       "                      prcssd_authors  \\\n",
       "paper_id                               \n",
       "1001                 [P S Aspinwall]   \n",
       "1002      [M Cvetic, H Lu, C N Pope]   \n",
       "1003             [Y S Myung, G Kang]   \n",
       "1004                    [A D Helfer]   \n",
       "1005         [J Fuchs, C Schweigert]   \n",
       "\n",
       "                                            prcssd_abstract  \\\n",
       "paper_id                                                      \n",
       "1001      [notes, based, lectures, given, tasi99, review...   \n",
       "1002      [point, massive, gauged, supergravity, potenti...   \n",
       "1003      [recently, ivanov, volovich, hep-th, 9912242, ...   \n",
       "1004      [quantum, fields, responding, moving, mirrors,...   \n",
       "1005      [proceedings, lie, iii, clausthal, july, 1999,...   \n",
       "\n",
       "                                                  neighbors  \n",
       "paper_id                                                     \n",
       "1001      9603161 9702094 9703082 9507158 9404151 950103...  \n",
       "1002      9912012 9812035 9902155 9606076 9809015 4158 9...  \n",
       "1003      205120 9409090 211182 9912242 208125 9709049 9...  \n",
       "1004      7129 9305170 9608190 209232 9910133 104118 990...  \n",
       "1005      9908025 9909114 4153 9805026 9307074 9607157 9...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_node_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info[\"neighbors\"] = df_node_info[\"neighbors\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>prcssd_title</th>\n",
       "      <th>prcssd_authors</th>\n",
       "      <th>prcssd_abstract</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[P S Aspinwall]</td>\n",
       "      <td>[notes, based, lectures, given, tasi99, review...</td>\n",
       "      <td>[9603161, 9702094, 9703082, 9507158, 9404151, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, walls, massive, gauged, supergravity,...</td>\n",
       "      <td>[M Cvetic, H Lu, C N Pope]</td>\n",
       "      <td>[point, massive, gauged, supergravity, potenti...</td>\n",
       "      <td>[9912012, 9812035, 9902155, 9606076, 9809015, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuations, brane, worlds]</td>\n",
       "      <td>[Y S Myung, G Kang]</td>\n",
       "      <td>[recently, ivanov, volovich, hep-th, 9912242, ...</td>\n",
       "      <td>[205120, 9409090, 211182, 9912242, 208125, 970...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[moving, mirrors, thermodynamic, paradoxes]</td>\n",
       "      <td>[A D Helfer]</td>\n",
       "      <td>[quantum, fields, responding, moving, mirrors,...</td>\n",
       "      <td>[7129, 9305170, 9608190, 209232, 9910133, 1041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundles, chiral, blocks, boundary, conditions...</td>\n",
       "      <td>[J Fuchs, C Schweigert]</td>\n",
       "      <td>[proceedings, lie, iii, clausthal, july, 1999,...</td>\n",
       "      <td>[9908025, 9909114, 4153, 9805026, 9307074, 960...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year                                              title  \\\n",
       "paper_id                                                            \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                              authors       journal_name  \\\n",
       "paper_id                                                   \n",
       "1001                Paul S. Aspinwall                      \n",
       "1002      M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "1003         Y.S. Myung, Gungwon Kang                      \n",
       "1004                   Adam D. Helfer          Phys.Rev.   \n",
       "1005          J. Fuchs, C. Schweigert                      \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "1001      these are notes based on lectures given at tas...   \n",
       "1002      we point out that massive gauged supergravity ...   \n",
       "1003      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "1004      quantum fields responding to moving mirrors ha...   \n",
       "1005      proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                                               prcssd_title  \\\n",
       "paper_id                                                      \n",
       "1001                  [compactification, geometry, duality]   \n",
       "1002      [domain, walls, massive, gauged, supergravity,...   \n",
       "1003         [comment, metric, fluctuations, brane, worlds]   \n",
       "1004            [moving, mirrors, thermodynamic, paradoxes]   \n",
       "1005      [bundles, chiral, blocks, boundary, conditions...   \n",
       "\n",
       "                      prcssd_authors  \\\n",
       "paper_id                               \n",
       "1001                 [P S Aspinwall]   \n",
       "1002      [M Cvetic, H Lu, C N Pope]   \n",
       "1003             [Y S Myung, G Kang]   \n",
       "1004                    [A D Helfer]   \n",
       "1005         [J Fuchs, C Schweigert]   \n",
       "\n",
       "                                            prcssd_abstract  \\\n",
       "paper_id                                                      \n",
       "1001      [notes, based, lectures, given, tasi99, review...   \n",
       "1002      [point, massive, gauged, supergravity, potenti...   \n",
       "1003      [recently, ivanov, volovich, hep-th, 9912242, ...   \n",
       "1004      [quantum, fields, responding, moving, mirrors,...   \n",
       "1005      [proceedings, lie, iii, clausthal, july, 1999,...   \n",
       "\n",
       "                                                  neighbors  \n",
       "paper_id                                                     \n",
       "1001      [9603161, 9702094, 9703082, 9507158, 9404151, ...  \n",
       "1002      [9912012, 9812035, 9902155, 9606076, 9809015, ...  \n",
       "1003      [205120, 9409090, 211182, 9912242, 208125, 970...  \n",
       "1004      [7129, 9305170, 9608190, 209232, 9910133, 1041...  \n",
       "1005      [9908025, 9909114, 4153, 9805026, 9307074, 960...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_node_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract embedding thanks to Bag of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict(zip(self.word2vec.keys(), range(len(self.word2vec.keys()))))\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        #self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        w1_embd = self.word2vec[w1]\n",
    "        w2_embd = self.word2vec[w2]\n",
    "        \n",
    "        cosine_sim = w1_embd.dot(w2_embd)/(np.linalg.norm(w1_embd)*np.linalg.norm(w2_embd))\n",
    "        return cosine_sim\n",
    "\n",
    "\n",
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                # number of words for which we have an embedding\n",
    "                n_words = sum([1 for word in sent if word in self.w2v.word2id])\n",
    "                \n",
    "                # compute sentence embedding only if we have the embedding of at least one word\n",
    "                if n_words > 0:\n",
    "                    # vector with the sum of all word embeddings element-wise\n",
    "                    sum_embd = np.sum(np.array([w2v.word2vec[word] for word in sent if word in self.w2v.word2id]), axis=0)\n",
    "                    mean_embd = sum_embd/n_words\n",
    "                    sentemb.append(np.array(mean_embd))\n",
    "                # if no embedding found, append a random embedding\n",
    "                else:\n",
    "                    sentemb.append(np.random.rand(300))\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                assert False, 'TODO: fill in the blank'\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)[0]\n",
    "        \n",
    "        scores = []\n",
    "        for key in keys:\n",
    "            score = self.score(query, key)\n",
    "            scores.append(score)\n",
    "        scores = np.array(scores)\n",
    "        argsorted_scores = np.argsort(scores)[::-1]\n",
    "        return [\" \".join(sentences[arg]) for arg in argsorted_scores[1:K+1]]\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        cosine_sim = s1.dot(s2)/(np.linalg.norm(s1)*np.linalg.norm(s2))\n",
    "        return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(\"data/crawl-300d-200k.vec\"), nmax=200000)\n",
    "s2v = BoV(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_node_info[\"abstract_embd\"] = [embd for embd in s2v.encode(df_node_info.prcssd_abstract)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study', 'two', 'loop', 'quantum', 'equivalence', 'sigma', 'models', 'related', \"buscher's\", 't-duality', 'transformation', 'computation', 'two', 'loop', 'perturbative', 'free', 'energy', 'density', 'performed', 'case', 'certain', 'deformation', 'su', '2', 'principal', 'sigma', 'model', 't-dual', 'using', 'dimensional', 'regularization', 'geometric', 'sigma', 'model', 'perturbation', 'theory', 'obtain', 'agreement', 'free', 'energy', 'density', 'expressions', 'two', 'models']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['matrix proposed two dimensional 3 sigma model dynamical theta term axion model exploiting abelian t-duality transformation connecting axion model integrable su 2 times u 1 symmetric principal sigma model strong evidence presented correctness proposed matrix comparing perturbatively calculated free energies ones based thermodynamical bethe ansatz t-duality transformation also leads new lax-pair models quantum non-integrability 3 sigma model sl constant theta term contradistinction axion model illustrated calculating 2 to3 particle production amplitude lowest order theta',\n",
       " 'modification abelian duality transformations proposed guaranteeing necessarily conformally invariant sigma model quantum equivalent least two loops perturbation theory dual requires somewhat non standard perturbative treatment sl dual sigma model explicit formulae modified duality transformation presented special class block diagonal purely metric sigma models']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = df_node_info.prcssd_abstract[20]\n",
    "print(abstract)\n",
    "s2v.most_similar(abstract, df_node_info.prcssd_abstract, K=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More insights on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 unique journals\n"
     ]
    }
   ],
   "source": [
    "# Number of unique journals\n",
    "n_unique = list(df_node_info.journal_name.unique())\n",
    "print(\"{} unique journals\".format(len(n_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these are notes based on lectures given at tasi99 we review the geometry of the moduli space of n 2 theories in four dimensions from the point of view of superstring compactification the cases of a type iia or type iib string compactified on a calabi-yau threefold and the heterotic string compactified on k3xt2 are each considered in detail we pay specific attention to the differences between n 2 theories and n 2 theories the moduli spaces of vector multiplets and the moduli spaces of hypermultiplets are reviewed in the case of hypermultiplets this review is limited by the poor state of our current understanding some peculiarities such as mixed instantons and the non-existence of a universal hypermultiplet are discussed'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One sample abstract\n",
    "df_node_info.abstract[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Commentaire Nabil : pas compris ce que ça fait là"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# random baseline #\n",
    "###################\n",
    "\n",
    "random_predictions = np.random.choice([0, 1], size=len(testing_set))\n",
    "random_predictions = zip(range(len(testing_set)),random_predictions)\n",
    "\n",
    "with open(\"predictions/random_predictions.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    for row in random_predictions:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "# note: Kaggle requires that you add \"ID\" and \"category\" column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beating random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the following shows how to construct a graph with igraph\n",
    "## even though in this baseline we don't use it\n",
    "## look at http://igraph.org/python/doc/igraph.Graph-class.html for feature ideas\n",
    "\n",
    "#edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "\n",
    "## some nodes may not be connected to any other node\n",
    "## hence the need to create the nodes of the graph from node_info.csv,\n",
    "## not just from the edge list\n",
    "\n",
    "#nodes = IDs\n",
    "\n",
    "## create empty directed graph\n",
    "#g = igraph.Graph(directed=True)\n",
    " \n",
    "## add vertices\n",
    "#g.add_vertices(nodes)\n",
    " \n",
    "## add edges\n",
    "#g.add_edges(edges)\n",
    "\n",
    "# for each training example we need to compute features\n",
    "# in this baseline we will train the model on only 5% of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# beating the random baseline #\n",
    "###############################\n",
    "\n",
    "# the following script gets an F1 score of approximately 0.66\n",
    "\n",
    "# data loading and preprocessing \n",
    "\n",
    "# the columns of the data frame below are: \n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# randomly select 5% of training set\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.05)))\n",
    "training_set_reduced = [training_set[i] for i in to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "# number of overlapping words in title\n",
    "overlap_title_test = []\n",
    "# temporal distance between the papers\n",
    "temp_diff_test = []\n",
    "# number of common authors\n",
    "comm_auth_test = []\n",
    "   \n",
    "counter = 0\n",
    "for i in range(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "\n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# issue predictions\n",
    "predictions_SVM = list(classifier.predict(testing_features))\n",
    "\n",
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "predictions_SVM = zip(range(len(testing_set)), predictions_SVM)\n",
    "\n",
    "with open(\"predictions/improved_predictions.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow((\"id\", \"category\"))\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**\n",
    "\n",
    "- Implement and test Adamic/Adar similarity **ok**\n",
    "- Implement and test Preferiential attachment **ok**\n",
    "- Implement and test Katz measure **ok, mais j'ai computé un score par noeud. Il me semble qu'il existe aussi un score par couple de neoud**\n",
    "- ... and the other features mentionned in slide 41 of lecture 5\n",
    "- Dans la fonction  `feature_engineer`, spécifier le nom des variables que l'on veut générer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join with node info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9510123'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_info_source = cp.deepcopy(df_node_info)\n",
    "df_node_info_source.columns = [col+\"_source\" for col in df_node_info.columns]\n",
    "df_train_inc = df_train.join(df_node_info_source, on=\"source\")\n",
    "\n",
    "df_node_info_target = cp.deepcopy(df_node_info)\n",
    "df_node_info_target.columns = [col+\"_target\" for col in df_node_info.columns]\n",
    "df_train_comp = df_train_inc.join(df_node_info_target, on=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "      <th>year_source</th>\n",
       "      <th>title_source</th>\n",
       "      <th>authors_source</th>\n",
       "      <th>journal_name_source</th>\n",
       "      <th>abstract_source</th>\n",
       "      <th>prcssd_title_source</th>\n",
       "      <th>prcssd_authors_source</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbors_source</th>\n",
       "      <th>year_target</th>\n",
       "      <th>title_target</th>\n",
       "      <th>authors_target</th>\n",
       "      <th>journal_name_target</th>\n",
       "      <th>abstract_target</th>\n",
       "      <th>prcssd_title_target</th>\n",
       "      <th>prcssd_authors_target</th>\n",
       "      <th>prcssd_abstract_target</th>\n",
       "      <th>neighbors_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>1995</td>\n",
       "      <td>an infinite number of potentials surrounding 2...</td>\n",
       "      <td></td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we found an infinite number of potentials surr...</td>\n",
       "      <td>[infinite, number, potentials, surrounding, 2d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9502114, 9408144, 9302075, 9411220, 108214, 9...</td>\n",
       "      <td>1995</td>\n",
       "      <td>stability analysis of the dilatonic black hole...</td>\n",
       "      <td>Won T. Kim, Julian Lee, , Young Jai Park</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we explicitly show that the net number of degr...</td>\n",
       "      <td>[stability, analysis, dilatonic, black, hole, ...</td>\n",
       "      <td>[W T Kim, J Lee, Y J Park]</td>\n",
       "      <td>[explicitly, show, net, number, degrees, freed...</td>\n",
       "      <td>[9201074, 9707129, 105264, 9210120, 9809016, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>1997</td>\n",
       "      <td>d 6 n 1 string vacua and duality</td>\n",
       "      <td>L.E.Ibanez, A.M.Uranga</td>\n",
       "      <td></td>\n",
       "      <td>winter school on duality mt sorak korea februa...</td>\n",
       "      <td>[6, n, 1, string, vacua, duality]</td>\n",
       "      <td>[L E Ibanez, A M Uranga]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9604178, 9603150, 9602097, 9203071, 103180, 9...</td>\n",
       "      <td>1996</td>\n",
       "      <td>strings on orientifolds</td>\n",
       "      <td>Atish Dabholkar, Jaemo Park</td>\n",
       "      <td>Nucl.Phys.</td>\n",
       "      <td>we construct several examples of compactificat...</td>\n",
       "      <td>[strings, orientifolds]</td>\n",
       "      <td>[A Dabholkar, J Park]</td>\n",
       "      <td>[construct, several, examples, compactificatio...</td>\n",
       "      <td>[9510209, 111018, 9509080, 9505054, 110084, 96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "      <td>conformal field theory and hyperbolic geometry</td>\n",
       "      <td>P. Kleban, I. Vassileva</td>\n",
       "      <td>Phys.Rev.Lett.</td>\n",
       "      <td>we examine the correspondence between the conf...</td>\n",
       "      <td>[conformal, field, theory, hyperbolic, geometry]</td>\n",
       "      <td>[P Kleban, I Vassileva]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9506142, 9705203, 12201, 209121, 9806107, 203...</td>\n",
       "      <td>1995</td>\n",
       "      <td>wightman functions behaviour on the event hori...</td>\n",
       "      <td>V. Moretti (Dept. Phys. Univ. Trento Italy, INFN)</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>reissner-nordstr om black hole results followi...</td>\n",
       "      <td>[wightman, functions, behaviour, event, horizo...</td>\n",
       "      <td>[V Moretti, INFN]</td>\n",
       "      <td>[reissner-nordstr, om, black, hole, results, f...</td>\n",
       "      <td>[9402155, 9811223, 9911148, 9608126, 9203019, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>comparing instanton contributions with exact r...</td>\n",
       "      <td>Philip C. Argyres, Sophie Pell,</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>supersymmetric scale invariant theories we dis...</td>\n",
       "      <td>[comparing, instanton, contributions, exact, r...</td>\n",
       "      <td>[P C Argyres, S Pell]</td>\n",
       "      <td>...</td>\n",
       "      <td>[302165, 9505100, 9607076, 9509175, 9609104, 9...</td>\n",
       "      <td>2003</td>\n",
       "      <td>five-dimensional gauge theories and quantum me...</td>\n",
       "      <td>Timothy J. Hollowood</td>\n",
       "      <td></td>\n",
       "      <td>we show how the dijkgraaf-vafa matrix model pr...</td>\n",
       "      <td>[five-dimensional, gauge, theories, quantum, m...</td>\n",
       "      <td>[T J Hollowood]</td>\n",
       "      <td>[show, dijkgraaf-vafa, matrix, model, proposal...</td>\n",
       "      <td>[9906011, 9911020, 9609239, 209223, 210239, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>quantum gravitational measure for three-geomet...</td>\n",
       "      <td></td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>that with some modifications was published in ...</td>\n",
       "      <td>[quantum, gravitational, measure, three-geomet...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[209076, 9407082, 9803168, 207213, 9610055, 10...</td>\n",
       "      <td>2002</td>\n",
       "      <td>israel conditions for the gauss-bonnet theory ...</td>\n",
       "      <td>Elias Gravanis, Steven Willison</td>\n",
       "      <td></td>\n",
       "      <td>on the brane universe additional bulk fields a...</td>\n",
       "      <td>[israel, conditions, gauss-bonnet, theory, fri...</td>\n",
       "      <td>[E Gravanis, S Willison]</td>\n",
       "      <td>[brane, universe, additional, bulk, fields, as...</td>\n",
       "      <td>[9704055, 109133, 9910149, 205299, 9611162, 99...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source   target  label  year_source  \\\n",
       "0  9510123  9502114      1         1995   \n",
       "1  9707075  9604178      1         1997   \n",
       "2  9312155  9506142      0         1993   \n",
       "3  9911255   302165      0         1999   \n",
       "4  9701033   209076      0         1997   \n",
       "\n",
       "                                        title_source  \\\n",
       "0  an infinite number of potentials surrounding 2...   \n",
       "1                   d 6 n 1 string vacua and duality   \n",
       "2     conformal field theory and hyperbolic geometry   \n",
       "3  comparing instanton contributions with exact r...   \n",
       "4  quantum gravitational measure for three-geomet...   \n",
       "\n",
       "                     authors_source journal_name_source  \\\n",
       "0                                            Phys.Lett.   \n",
       "1            L.E.Ibanez, A.M.Uranga                       \n",
       "2           P. Kleban, I. Vassileva      Phys.Rev.Lett.   \n",
       "3  Philip C. Argyres, Sophie Pell,                 JHEP   \n",
       "4                                            Phys.Lett.   \n",
       "\n",
       "                                     abstract_source  \\\n",
       "0  we found an infinite number of potentials surr...   \n",
       "1  winter school on duality mt sorak korea februa...   \n",
       "2  we examine the correspondence between the conf...   \n",
       "3  supersymmetric scale invariant theories we dis...   \n",
       "4  that with some modifications was published in ...   \n",
       "\n",
       "                                 prcssd_title_source  \\\n",
       "0  [infinite, number, potentials, surrounding, 2d...   \n",
       "1                  [6, n, 1, string, vacua, duality]   \n",
       "2   [conformal, field, theory, hyperbolic, geometry]   \n",
       "3  [comparing, instanton, contributions, exact, r...   \n",
       "4  [quantum, gravitational, measure, three-geomet...   \n",
       "\n",
       "      prcssd_authors_source  \\\n",
       "0                        []   \n",
       "1  [L E Ibanez, A M Uranga]   \n",
       "2   [P Kleban, I Vassileva]   \n",
       "3     [P C Argyres, S Pell]   \n",
       "4                        []   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                    neighbors_source year_target  \\\n",
       "0  [9502114, 9408144, 9302075, 9411220, 108214, 9...        1995   \n",
       "1  [9604178, 9603150, 9602097, 9203071, 103180, 9...        1996   \n",
       "2  [9506142, 9705203, 12201, 209121, 9806107, 203...        1995   \n",
       "3  [302165, 9505100, 9607076, 9509175, 9609104, 9...        2003   \n",
       "4  [209076, 9407082, 9803168, 207213, 9610055, 10...        2002   \n",
       "\n",
       "                                        title_target  \\\n",
       "0  stability analysis of the dilatonic black hole...   \n",
       "1                            strings on orientifolds   \n",
       "2  wightman functions behaviour on the event hori...   \n",
       "3  five-dimensional gauge theories and quantum me...   \n",
       "4  israel conditions for the gauss-bonnet theory ...   \n",
       "\n",
       "                                      authors_target journal_name_target  \\\n",
       "0           Won T. Kim, Julian Lee, , Young Jai Park          Phys.Lett.   \n",
       "1                        Atish Dabholkar, Jaemo Park          Nucl.Phys.   \n",
       "2  V. Moretti (Dept. Phys. Univ. Trento Italy, INFN)   Class.Quant.Grav.   \n",
       "3                               Timothy J. Hollowood                       \n",
       "4                    Elias Gravanis, Steven Willison                       \n",
       "\n",
       "                                     abstract_target  \\\n",
       "0  we explicitly show that the net number of degr...   \n",
       "1  we construct several examples of compactificat...   \n",
       "2  reissner-nordstr om black hole results followi...   \n",
       "3  we show how the dijkgraaf-vafa matrix model pr...   \n",
       "4  on the brane universe additional bulk fields a...   \n",
       "\n",
       "                                 prcssd_title_target  \\\n",
       "0  [stability, analysis, dilatonic, black, hole, ...   \n",
       "1                            [strings, orientifolds]   \n",
       "2  [wightman, functions, behaviour, event, horizo...   \n",
       "3  [five-dimensional, gauge, theories, quantum, m...   \n",
       "4  [israel, conditions, gauss-bonnet, theory, fri...   \n",
       "\n",
       "        prcssd_authors_target  \\\n",
       "0  [W T Kim, J Lee, Y J Park]   \n",
       "1       [A Dabholkar, J Park]   \n",
       "2           [V Moretti, INFN]   \n",
       "3             [T J Hollowood]   \n",
       "4    [E Gravanis, S Willison]   \n",
       "\n",
       "                              prcssd_abstract_target  \\\n",
       "0  [explicitly, show, net, number, degrees, freed...   \n",
       "1  [construct, several, examples, compactificatio...   \n",
       "2  [reissner-nordstr, om, black, hole, results, f...   \n",
       "3  [show, dijkgraaf-vafa, matrix, model, proposal...   \n",
       "4  [brane, universe, additional, bulk, fields, as...   \n",
       "\n",
       "                                    neighbors_target  \n",
       "0  [9201074, 9707129, 105264, 9210120, 9809016, 9...  \n",
       "1  [9510209, 111018, 9509080, 9505054, 110084, 96...  \n",
       "2  [9402155, 9811223, 9911148, 9608126, 9203019, ...  \n",
       "3  [9906011, 9911020, 9609239, 209223, 210239, 20...  \n",
       "4  [9704055, 109133, 9910149, 205299, 9611162, 99...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common words in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_texts(t1, t2):\n",
    "    n_overlapping_words = sum([1 for token in t1 if token in t2])\n",
    "    return n_overlapping_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"overlap_title\"] = df_train_comp.apply(lambda row: compare_texts(row[\"prcssd_title_source\"], row[\"prcssd_title_target\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common word in abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"overlap_abstract\"] = df_train_comp.apply(\n",
    "    lambda row: compare_texts(row[\"prcssd_abstract_source\"], row[\"prcssd_abstract_target\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_comp[\"cosine_sim_abstract\"] = df_train_comp.apply(\n",
    "    lambda row: s2v.score(row[\"abstract_embd_source\"], row[\"abstract_embd_target\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_common_authors(l_authors1, l_authors2):\n",
    "    n_common_authors = sum([1 for author in l_authors1 if author in l_authors2])\n",
    "    return n_common_authors\n",
    "\n",
    "# Commenataire Nabil : plus de sens de mettre le ratio de common authors ? haha petit détail, pense pas que ça va\n",
    "# changer grand chose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"common_authors\"] = df_train_comp.apply(\n",
    "    lambda row: count_common_authors(row[\"prcssd_authors_source\"], row[\"prcssd_authors_target\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"temp_diff\"] = df_train_comp.apply(\n",
    "    lambda row: row[\"year_source\"]-row[\"year_target\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among lines that have ```temp_diff < 0```, how many have ```label = 0``` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion: 98.36%\n"
     ]
    }
   ],
   "source": [
    "n_temp_lt_0 = df_train_comp[df_train_comp.temp_diff < 0].shape[0]\n",
    "n_both = df_train_comp[(df_train_comp.temp_diff < 0) & (df_train_comp.label == 0)].shape[0]\n",
    "print(\"Proportion: {:.2%}\".format(n_both/n_temp_lt_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will always predict 0 for lines that have ```temp_diff < 0```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the source and target belong to the same journal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"is_same_journal\"] = ((df_train_comp.journal_name_source == df_train_comp.journal_name_target) \n",
    "                                    & (df_train_comp.journal_name_source != \"\")).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the relevance of the `common_authors` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    569972\n",
       "1     30602\n",
       "2     10957\n",
       "3      3214\n",
       "4       672\n",
       "5        87\n",
       "6         6\n",
       "8         2\n",
       "Name: common_authors, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_comp.common_authors.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(neighbors_target, neighbors_source):\n",
    "    \"\"\"Compute the jaddart coefficient between a source and a target node. \n",
    "    The neighbors are represented by a list of strings.\n",
    "    \"\"\"\n",
    "    common_neighbors = len(set(neighbors_source).intersection(set(neighbors_target)))\n",
    "    total_neighbors = len(neighbors_source+neighbors_target)\n",
    "    return common_neighbors/total_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"jaccard_coefficient\"] = df_train_comp.apply(\n",
    "    lambda row: jaccard_coefficient(row[\"neighbors_source\"], row[\"neighbors_target\"]), \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adamic/Adar similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION** Notre graph G est normalement un graph direct. Or, on ne peut pas calculer adamic adar similarity si le graph est direct. On passe alors par un grap indirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the nodes from the source and target columns\n",
    "G.add_nodes_from(df_train_comp['source'])\n",
    "G.add_nodes_from(df_train_comp['target'])\n",
    "\n",
    "# Add the edges\n",
    "edges = [(df_train_comp['source'][i], df_train_comp['target'][i]) for i in range(len(df_train_comp))\n",
    "        if df_train_comp['label'][i] == 1]\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compute = [(df_train_comp['source'][i], df_train_comp['target'][i]) for i in range(len(df_train_comp))]\n",
    "preds = nx.adamic_adar_index(G, to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dico = {(source, target): sim for (source, target, sim) in preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"AAsim\"] = df_train_comp.apply(\n",
    "    lambda row: preds_dico[(row[\"source\"], row[\"target\"])], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preferential attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compute = [(df_train_comp['source'][i], df_train_comp['target'][i]) for i in range(len(df_train_comp))]\n",
    "preds_pa = nx.preferential_attachment(G, to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pa_dico = {(source, target): sim for (source, target, sim) in preds_pa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"pref_attachment\"] = df_train_comp.apply(\n",
    "    lambda row: preds_pa_dico[(row[\"source\"], row[\"target\"])], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Katz centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj_matrix = np.array(nx.adjacency_matrix(G).todense())\n",
    "#phi = max(np.linalg.eigvals(adj_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-179>\u001b[0m in \u001b[0;36mkatz_centrality\u001b[1;34m(G, alpha, beta, max_iter, tol, nstart, normalized, weight)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m                                             ' '.join(graph_types))\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\centrality\\katz.py\u001b[0m in \u001b[0;36mkatz_centrality\u001b[1;34m(G, alpha, beta, max_iter, tol, nstart, normalized, weight)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mnbr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnbr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mxlast\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnbr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#centrality_dico = nx.katz_centrality(G, 1/phi)\n",
    "%time centrality_dico = nx.katz_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_comp[\"katz_centrality_source\"] = df_train_comp.apply(\n",
    "    lambda row: centrality_dico[row[\"source\"]], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"katz_centrality_target\"] = df_train_comp.apply(\n",
    "    lambda row: centrality_dico[row[\"target\"]], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Has path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a directed Graph\n",
    "G_dir = nx.DiGraph()\n",
    "\n",
    "# Add the nodes from the source and target columns\n",
    "G_dir.add_nodes_from(df_train_comp['source'])\n",
    "G_dir.add_nodes_from(df_train_comp['target'])\n",
    "\n",
    "# Add the edges\n",
    "edges = [(df_train_comp['source'][i], df_train_comp['target'][i]) for i in range(len(df_train_comp))\n",
    "        if df_train_comp['label'][i] == 1]\n",
    "G_dir.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4852\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4853\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4854\u001b[1;33m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[0;32m   4855\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4856\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4908\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4909\u001b[0m                     result = lib.reduce(values, func, axis=axis, dummy=dummy,\n\u001b[1;32m-> 4910\u001b[1;33m                                         labels=labels)\n\u001b[0m\u001b[0;32m   4911\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4912\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.reduce\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\shortest_paths\\generic.py\u001b[0m in \u001b[0;36mhas_path\u001b[1;34m(G, source, target)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortest_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetworkXNoPath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\shortest_paths\\generic.py\u001b[0m in \u001b[0;36mshortest_path\u001b[1;34m(G, source, target, weight)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;31m# Find shortest source-target path.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbidirectional_shortest_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdijkstra_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\shortest_paths\\unweighted.py\u001b[0m in \u001b[0;36mbidirectional_shortest_path\u001b[1;34m(G, source, target)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;31m# call helper to do the real work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_bidirectional_pred_succ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\shortest_paths\\unweighted.py\u001b[0m in \u001b[0;36m_bidirectional_pred_succ\u001b[1;34m(G, source, target)\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msucc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                         \u001b[0msucc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                         \u001b[0mreverse_fringe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# found path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df_train_comp[\"has_path\"] = df_train_comp.apply(\n",
    "    lambda row: int(nx.has_path(G_dir,row[\"source\"],row[\"target\"])),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lenght of the shortest path between 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_comp[\"shortest_path\"] = df_train_comp.apply(\n",
    "    lambda row: nx.shortest_path_length(G_dir,source = row[\"source\"],target = row[\"target\"])\n",
    "    if row[\"has_path\"] == 1\n",
    "    else -1,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_comp[\"common_ngbrs\"] = df_train_comp.apply(\n",
    "    lambda row: len(list(nx.common_neighbors(G, row[\"source\"], row[\"target\"]))),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkXUnbounded",
     "evalue": "('Infinite capacity path, flow unbounded above.', 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNetworkXUnbounded\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-dc329d94e1b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m df_train_comp[\"max_flow_value\"] = df_train_comp.apply(\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum_flow_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     axis=1)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4852\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4853\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4854\u001b[1;33m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[0;32m   4855\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4856\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4948\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4949\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4950\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4951\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4952\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-dc329d94e1b5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m df_train_comp[\"max_flow_value\"] = df_train_comp.apply(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum_flow_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     axis=1)\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\flow\\maxflow.py\u001b[0m in \u001b[0;36mmaximum_flow_value\u001b[1;34m(G, s, t, capacity, flow_func, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetworkXError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"flow_func has to be callable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m     \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flow_value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\flow\\preflowpush.py\u001b[0m in \u001b[0;36mpreflow_push\u001b[1;34m(G, s, t, capacity, residual, global_relabel_freq, value_only)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \"\"\"\n\u001b[0;32m    427\u001b[0m     R = preflow_push_impl(G, s, t, capacity, residual, global_relabel_freq,\n\u001b[1;32m--> 428\u001b[1;33m                           value_only)\n\u001b[0m\u001b[0;32m    429\u001b[0m     \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'algorithm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'preflow_push'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\flow\\preflowpush.py\u001b[0m in \u001b[0;36mpreflow_push_impl\u001b[1;34m(G, s, t, capacity, residual, global_relabel_freq, value_only)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mdetect_unboundedness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mR_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\networkx\\algorithms\\flow\\utils.py\u001b[0m in \u001b[0;36mdetect_unboundedness\u001b[1;34m(R, s, t)\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                     raise nx.NetworkXUnbounded(\n\u001b[1;32m--> 157\u001b[1;33m                         'Infinite capacity path, flow unbounded above.')\n\u001b[0m\u001b[0;32m    158\u001b[0m                 \u001b[0mseen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNetworkXUnbounded\u001b[0m: ('Infinite capacity path, flow unbounded above.', 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "df_train_comp[\"max_flow_value\"] = df_train_comp.apply(\n",
    "    lambda row: nx.maximum_flow_value(G, row[\"source\"], row[\"target\"]),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `feature_engineer` function we add new features to the initial dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph as an undirected graph\n",
    "#Initiate the Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the nodes from the source and target columns\n",
    "G.add_nodes_from(df_train_comp['source'])\n",
    "G.add_nodes_from(df_train_comp['target'])\n",
    "\n",
    "# Add the edges\n",
    "edges = [(df_train_comp['source'][i], df_train_comp['target'][i]) for i in range(len(df_train_comp))\n",
    "        if df_train_comp['label'][i] == 1]\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph as an directed graph\n",
    "#Initiate a direct Graph\n",
    "G_dir = nx.DiGraph()\n",
    "\n",
    "# Add the nodes from the source and target columns\n",
    "G_dir.add_nodes_from(df_train_comp['source'])\n",
    "G_dir.add_nodes_from(df_train_comp['target'])\n",
    "\n",
    "# Add the edges\n",
    "edges = [(df_train_comp['source'][i], df_train_comp['target'][i]) for i in range(len(df_train_comp))\n",
    "        if df_train_comp['label'][i] == 1]\n",
    "G_dir.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_texts(t1, t2):\n",
    "    n_overlapping_words = sum([1 for token in t1 if token in t2])\n",
    "    return n_overlapping_words\n",
    "\n",
    "def count_common_authors(l_authors1, l_authors2):\n",
    "    n_common_authors = sum([1 for author in l_authors1 if author in l_authors2])\n",
    "    return n_common_authors\n",
    "\n",
    "def jaccard_coefficient(neighbors_target, neighbors_source):\n",
    "    \"\"\"Compute the jaddart coefficient between a source and a target node. \n",
    "    The neighbors are represented by a list of strings.\n",
    "    \"\"\"\n",
    "    common_neighbors = len(set(neighbors_source).intersection(set(neighbors_target)))\n",
    "    total_neighbors = len(neighbors_source+neighbors_target)\n",
    "    return common_neighbors/total_neighbors\n",
    "\n",
    "def AAsim(df_comp, G):\n",
    "    to_compute = [(df_comp['source'][i], df_comp['target'][i]) for i in range(len(df_comp))] \n",
    "    preds = nx.adamic_adar_index(G, to_compute)\n",
    "    \n",
    "    preds_dico = {(source, target): sim for (source, target, sim) in preds}\n",
    "    \n",
    "    return preds_dico\n",
    "\n",
    "def prefatt(df_comp, G):\n",
    "    to_compute = [(df_comp['source'][i], df_comp['target'][i]) for i in range(len(df_comp))]\n",
    "    preds_pa = nx.preferential_attachment(G, to_compute)\n",
    "    \n",
    "    preds_pa_dico = {(source, target): sim for (source, target, sim) in preds_pa}\n",
    "    \n",
    "    return preds_pa_dico\n",
    "\n",
    "def Katz_centrality(G):\n",
    "    centrality_dico = nx.katz_centrality(G)\n",
    "    \n",
    "    return centrality_dico\n",
    "\n",
    "# Has path ok\n",
    "#Shortest path ok\n",
    "#Common neighbors ok\n",
    "# Max flow value ok\n",
    "\n",
    "# All previous feature engineering in one function\n",
    "def feature_engineer(df, df_node_info, features=None):\n",
    "    \"\"\"Specify the list of features to compute in features.\n",
    "    \"\"\"\n",
    "    # Join\n",
    "    df_node_info_source = cp.deepcopy(df_node_info)\n",
    "    df_node_info_source.columns = [col+\"_source\" for col in df_node_info.columns]\n",
    "    df_inc = df.join(df_node_info_source, on=\"source\")\n",
    "    \n",
    "    df_node_info_target = cp.deepcopy(df_node_info)\n",
    "    df_node_info_target.columns = [col+\"_target\" for col in df_node_info.columns]\n",
    "    df_comp = df_inc.join(df_node_info_target, on=\"target\")\n",
    "    print(\"Joining: ok!\")\n",
    "    \n",
    "    # Common words in title\n",
    "    if \"title\" in features or features is None:\n",
    "        df_comp[\"overlap_title\"] = df_comp.apply(\n",
    "            lambda row: compare_texts(row[\"prcssd_title_source\"], row[\"prcssd_title_target\"]), \n",
    "            axis=1)\n",
    "        print(\"Common words in title: ok!\")\n",
    "    \n",
    "    # Number of common authors\n",
    "    if \"title\" in features or features is None:\n",
    "        df_comp[\"common_authors\"] = df_comp.apply(\n",
    "            lambda row: count_common_authors(row[\"prcssd_authors_source\"], row[\"prcssd_authors_target\"]), \n",
    "            axis=1)\n",
    "        print(\"Common authors: ok!\")\n",
    "    \n",
    "    # Temporal difference\n",
    "    if \"temp\" in features or features is None:\n",
    "        df_comp[\"temp_diff\"] = df_comp.apply(\n",
    "            lambda row: row[\"year_source\"]-row[\"year_target\"], axis=1)\n",
    "        print(\"Temporal difference: ok!\")\n",
    "\n",
    "    # Common words in abstract\n",
    "    if \"abstract\" in features or features is None:\n",
    "        df_comp[\"overlap_abstract\"] = df_comp.apply(\n",
    "            lambda row: compare_texts(row[\"prcssd_abstract_source\"], row[\"prcssd_abstract_target\"]), \n",
    "            axis=1)\n",
    "        print(\"Common words in abstract: ok!\")\n",
    "    \n",
    "    if \"cosine_abstract\" in features or features is None:\n",
    "        df_comp[\"cosine_sim_abstract\"] = df_comp.\\\n",
    "                                 apply(lambda row: s2v.score(row[\"abstract_embd_source\"], \n",
    "                                                     row[\"abstract_embd_target\"]), axis=1)\n",
    "        print(\"Cosine similarity abstract: ok!\")\n",
    "    \n",
    "    # Source and target belong to the same journal ?\n",
    "    if \"journal\" in features or features is None:\n",
    "        df_comp[\"is_same_journal\"] = ((df_comp.journal_name_source == df_comp.journal_name_target) & (df_comp.journal_name_source != \"\")).astype(\"int\")\n",
    "        print(\"Same journal: ok!\")\n",
    "    \n",
    "    if \"jaccard\" in features or features is None:\n",
    "        df_comp[\"jaccard_coefficient\"] = df_comp.apply(\n",
    "        lambda row: jaccard_coefficient(row[\"neighbors_source\"], row[\"neighbors_target\"]), \n",
    "        axis=1)\n",
    "        print(\"Jaccard Coefficient: ok!\")\n",
    "    \n",
    "    # Adamic Adar similarity\n",
    "    if \"addar\" in features or features is None:\n",
    "        preds_dico = AAsim(df_comp, G)\n",
    "        df_comp[\"AAsim\"] = df_comp.apply(\n",
    "        lambda row: preds_dico[(row[\"source\"], row[\"target\"])], \n",
    "        axis=1)\n",
    "        print(\"Adamic Adar similarity: ok!\")\n",
    "    \n",
    "    # Preferential attachment\n",
    "    if \"pr\" in features or features is None:\n",
    "        preds_pa_dico = prefatt(df_comp, G)\n",
    "        df_comp[\"pref_attachment\"] = df_comp.apply(\n",
    "        lambda row: preds_pa_dico[(row[\"source\"], row[\"target\"])], \n",
    "        axis=1)\n",
    "        print(\"Preferential attachment: ok!\")\n",
    "\n",
    "    # Katz centrality (très long)\n",
    "    if \"katz\" in features or features is None:\n",
    "        centrality_dico = Katz_centrality(G)\n",
    "        df_comp[\"katz_centrality_source\"] = df_comp.apply(\n",
    "        lambda row: centrality_dico[row[\"source\"]], \n",
    "        axis=1)\n",
    "        df_comp[\"katz_centrality_target\"] = df_comp.apply(\n",
    "        lambda row: centrality_dico[row[\"target\"]], \n",
    "        axis=1)\n",
    "        print(\"Katz centrality: ok!\")\n",
    "    \n",
    "    # Has path (long)\n",
    "    if \"has_path\" in features or features is None:\n",
    "        df_comp[\"has_path\"] = df_comp.apply(\n",
    "        lambda row: int(nx.has_path(G_dir,row[\"source\"],row[\"target\"])),\n",
    "        axis=1)\n",
    "        print(\"Has path: ok\")\n",
    "    \n",
    "    # Shortest path (long)\n",
    "    if \"shortest_path\" in features or features is None:\n",
    "        df_train_comp[\"shortest_path\"] = df_train_comp.apply(\n",
    "        lambda row: nx.shortest_path_length(G_dir,source = row[\"source\"],target = row[\"target\"])\n",
    "        if row[\"has_path\"] == 1\n",
    "        else -1,\n",
    "        axis=1)\n",
    "        print(\"Shortest path: ok!\")\n",
    "\n",
    "    # Common neighbors\n",
    "    if \"common_neighbors\" in features or features is None:\n",
    "        df_train_comp[\"common_ngbrs\"] = df_train_comp.apply(\n",
    "            lambda row: len(list(nx.common_neighbors(G, row[\"source\"], row[\"target\"]))),\n",
    "            axis=1)\n",
    "        print(\"Common neighbors: ok!\")\n",
    "    \n",
    "    # Max flow values\n",
    "    if \"max_flow\" in features or features is None:\n",
    "        df_comp[\"max_flow_value\"] = df_comp.apply(\n",
    "        lambda row: nx.maximum_flow_value(G, row[\"source\"], row[\"target\"]),\n",
    "        axis=1)\n",
    "        print(\"Max flow values: ok!\")\n",
    "    \n",
    "    \n",
    "    return df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining: ok!\n",
      "Common words in title: ok!\n",
      "Common authors: ok!\n",
      "Temporal difference: ok!\n",
      "Common words in abstract: ok!\n",
      "Cosine similarity abstract: ok!\n",
      "Same journal: ok!\n",
      "Jaccard Coefficient: ok!\n",
      "Adamic Adar similarity: ok!\n",
      "Preferential attachment: ok!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "(\"object of type 'generator' has no len()\", 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-903e16d15ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train_comp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_node_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-222a2eb4b4c9>\u001b[0m in \u001b[0;36mfeature_engineer\u001b[0;34m(df, df_node_info)\u001b[0m\n\u001b[1;32m    128\u001b[0m     df_comp[\"common_ngbrs\"] = df_comp.apply(\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     axis=1)\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Common neighbors: ok!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4260\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4262\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4357\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4358\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4359\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4360\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-222a2eb4b4c9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# Common neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     df_comp[\"common_ngbrs\"] = df_comp.apply(\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     axis=1)\n\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Common neighbors: ok!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: (\"object of type 'generator' has no len()\", 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "df_train_comp = feature_engineer(df_train, df_node_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "      <th>year_source</th>\n",
       "      <th>title_source</th>\n",
       "      <th>authors_source</th>\n",
       "      <th>journal_name_source</th>\n",
       "      <th>abstract_source</th>\n",
       "      <th>prcssd_title_source</th>\n",
       "      <th>prcssd_authors_source</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbors_target</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>is_same_journal</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>AAsim</th>\n",
       "      <th>pref_attachment</th>\n",
       "      <th>common_ngbrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>1995</td>\n",
       "      <td>an infinite number of potentials surrounding 2...</td>\n",
       "      <td></td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we found an infinite number of potentials surr...</td>\n",
       "      <td>[infinite, number, potentials, surrounding, 2d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9201074, 9707129, 105264, 9210120, 9809016, 9...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.513898</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>1997</td>\n",
       "      <td>d 6 n 1 string vacua and duality</td>\n",
       "      <td>L.E.Ibanez, A.M.Uranga</td>\n",
       "      <td></td>\n",
       "      <td>winter school on duality mt sorak korea februa...</td>\n",
       "      <td>[6, n, 1, string, vacua, duality]</td>\n",
       "      <td>[L E Ibanez, A M Uranga]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9510209, 111018, 9509080, 9505054, 110084, 96...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>4.320366</td>\n",
       "      <td>11613</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "      <td>conformal field theory and hyperbolic geometry</td>\n",
       "      <td>P. Kleban, I. Vassileva</td>\n",
       "      <td>Phys.Rev.Lett.</td>\n",
       "      <td>we examine the correspondence between the conf...</td>\n",
       "      <td>[conformal, field, theory, hyperbolic, geometry]</td>\n",
       "      <td>[P Kleban, I Vassileva]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9402155, 9811223, 9911148, 9608126, 9203019, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>comparing instanton contributions with exact r...</td>\n",
       "      <td>Philip C. Argyres, Sophie Pell,</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>supersymmetric scale invariant theories we dis...</td>\n",
       "      <td>[comparing, instanton, contributions, exact, r...</td>\n",
       "      <td>[P C Argyres, S Pell]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9906011, 9911020, 9609239, 209223, 210239, 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>quantum gravitational measure for three-geomet...</td>\n",
       "      <td></td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>that with some modifications was published in ...</td>\n",
       "      <td>[quantum, gravitational, measure, three-geomet...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9704055, 109133, 9910149, 205299, 9611162, 99...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source   target  label  year_source  \\\n",
       "0  9510123  9502114      1         1995   \n",
       "1  9707075  9604178      1         1997   \n",
       "2  9312155  9506142      0         1993   \n",
       "3  9911255   302165      0         1999   \n",
       "4  9701033   209076      0         1997   \n",
       "\n",
       "                                        title_source  \\\n",
       "0  an infinite number of potentials surrounding 2...   \n",
       "1                   d 6 n 1 string vacua and duality   \n",
       "2     conformal field theory and hyperbolic geometry   \n",
       "3  comparing instanton contributions with exact r...   \n",
       "4  quantum gravitational measure for three-geomet...   \n",
       "\n",
       "                     authors_source journal_name_source  \\\n",
       "0                                            Phys.Lett.   \n",
       "1            L.E.Ibanez, A.M.Uranga                       \n",
       "2           P. Kleban, I. Vassileva      Phys.Rev.Lett.   \n",
       "3  Philip C. Argyres, Sophie Pell,                 JHEP   \n",
       "4                                            Phys.Lett.   \n",
       "\n",
       "                                     abstract_source  \\\n",
       "0  we found an infinite number of potentials surr...   \n",
       "1  winter school on duality mt sorak korea februa...   \n",
       "2  we examine the correspondence between the conf...   \n",
       "3  supersymmetric scale invariant theories we dis...   \n",
       "4  that with some modifications was published in ...   \n",
       "\n",
       "                                 prcssd_title_source  \\\n",
       "0  [infinite, number, potentials, surrounding, 2d...   \n",
       "1                  [6, n, 1, string, vacua, duality]   \n",
       "2   [conformal, field, theory, hyperbolic, geometry]   \n",
       "3  [comparing, instanton, contributions, exact, r...   \n",
       "4  [quantum, gravitational, measure, three-geomet...   \n",
       "\n",
       "      prcssd_authors_source     ...       \\\n",
       "0                        []     ...        \n",
       "1  [L E Ibanez, A M Uranga]     ...        \n",
       "2   [P Kleban, I Vassileva]     ...        \n",
       "3     [P C Argyres, S Pell]     ...        \n",
       "4                        []     ...        \n",
       "\n",
       "                                    neighbors_target overlap_title  \\\n",
       "0  [9201074, 9707129, 105264, 9210120, 9809016, 9...             2   \n",
       "1  [9510209, 111018, 9509080, 9505054, 110084, 96...             0   \n",
       "2  [9402155, 9811223, 9911148, 9608126, 9203019, ...             0   \n",
       "3  [9906011, 9911020, 9609239, 209223, 210239, 20...             0   \n",
       "4  [9704055, 109133, 9910149, 205299, 9611162, 99...             0   \n",
       "\n",
       "   overlap_abstract common_authors temp_diff is_same_journal  \\\n",
       "0                 6              0         0               1   \n",
       "1                 6              0         1               0   \n",
       "2                 4              0        -2               0   \n",
       "3                 6              0        -4               0   \n",
       "4                 5              0        -5               0   \n",
       "\n",
       "  jaccard_coefficient     AAsim pref_attachment common_ngbrs  \n",
       "0            0.062500  0.513898              72            1  \n",
       "1            0.069565  4.320366           11613           20  \n",
       "2            0.000000  0.000000               5            0  \n",
       "3            0.000000  0.000000             280            0  \n",
       "4            0.000000  0.000000             168            0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"year_source\", \"overlap_title\", \"common_authors\", \"temp_diff\", \"overlap_abstract\",\n",
    "            \"is_same_journal\", \"jaccard_coefficient\", \"AAsim\", \"pref_attachment\", \n",
    "            \"common_ngbrs\"]\n",
    "# Train only on data with temp_diff > 0\n",
    "X = df_train_comp[features]\n",
    "y = df_train_comp.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "# Set a k-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.2f}\"\\\n",
    "    .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the feature importances for the Random Forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature common_ngbrs (0.344697)\n",
      "2. feature jaccard_coefficient (0.264291)\n",
      "3. feature AAsim (0.256332)\n",
      "4. feature pref_attachment (0.055488)\n",
      "5. feature temp_diff (0.029636)\n",
      "6. feature overlap_abstract (0.015910)\n",
      "7. feature common_authors (0.014633)\n",
      "8. feature year_source (0.011561)\n",
      "9. feature overlap_title (0.006396)\n",
      "10. feature is_same_journal (0.001057)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFJCAYAAABzZy3XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269011c13c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the clf\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), [features[index] for index in indices], rotation=45)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross-validation F1: 0.97\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=10)\n",
    "# Set a k-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.2f}\"\\\n",
    "    .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Mean of cross-validation F1: 0.80\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, verbose=1)\n",
    "# Set a k-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "# Get average cross-validation on 5 folds\n",
    "print(\"Mean of cross-validation F1: {:.2f}\"\\\n",
    "    .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "    .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = SVC()\n",
    "# # Set a k-fold cross validation\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "# # Get average cross-validation on 5 folds\n",
    "# print(\"Mean of cross-validation F1: {:.2f}\"\\\n",
    "#     .format(cross_val_score(clf, X, y, cv=cv, scoring='f1')\\\n",
    "#     .mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining: ok!\n",
      "Common words in title: ok!\n",
      "Common authors: ok!\n",
      "Temporal difference: ok!\n",
      "Common words in abstract: ok!\n",
      "Cosine similarity abstract: ok!\n",
      "Same journal: ok!\n",
      "Jaccard Coefficient: ok!\n",
      "Adamic Adar similarity: ok!\n"
     ]
    }
   ],
   "source": [
    "df_test_comp = feature_engineer(df_test, df_node_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=10)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(df, predictions_unc):\n",
    "    predictions = []\n",
    "    idx_pred = 0\n",
    "    for row in df.iterrows():\n",
    "        if row[1][\"temp_diff\"] < 0:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(predictions_unc[idx_pred])\n",
    "            idx_pred+=1\n",
    "    return predictions\n",
    "\n",
    "predictions = list(clf.predict(df_test_comp[features][df_test_comp.temp_diff >= 0]))\n",
    "predictions = pred(df_test_comp[features], predictions)\n",
    "predictions = zip(range(len(testing_set)), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = date.today().isoformat()\n",
    "name_file = \"AAsim_{}.csv\".format(dt)\n",
    "with open(\"predictions/{}\".format(name_file),\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow((\"id\", \"category\"))\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
